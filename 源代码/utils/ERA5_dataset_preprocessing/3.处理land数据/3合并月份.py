import xarray as xr
import numpy as np
import os

def merge_land_data_safe():
    """
    å®‰å…¨åˆå¹¶landæ•°æ®æ–‡ä»¶ - é€ä¸ªæ–‡ä»¶å¤„ç†ç¡®ä¿æ•°æ®å®Œæ•´æ€§
    """
    # æ˜ç¡®åˆ—å‡ºæ‰€æœ‰è¦åˆå¹¶çš„æ–‡ä»¶
    file_list = [
        'land_merged_2024-03.nc',
        'land_merged_2024-04.nc', 
        'land_merged_2024-05.nc',
        'land_merged_2024-06.nc',
        'land_merged_2024-07.nc',
        'land_merged_2024-08.nc',
        'land_merged_2024-09.nc',
        'land_merged_2024-10.nc',
        'land_merged_2024-11.nc',
        'land_merged_2024-12.nc',
        'land_merged_2025-01.nc',
        'land_merged_2025-02.nc'
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶è·å–åŸºæœ¬ä¿¡æ¯
    existing_files = []
    file_sizes = []
    
    for file in file_list:
        if os.path.exists(file):
            file_size = os.path.getsize(file) / 1024  # KB
            existing_files.append(file)
            file_sizes.append(file_size)
            print(f"âœ… {file} ({file_size:.1f} KB)")
        else:
            print(f"âŒ {file} (ç¼ºå¤±)")
    
    if not existing_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ–‡ä»¶")
        return None
    
    print(f"\næ‰¾åˆ° {len(existing_files)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å°: {sum(file_sizes):.1f} KB")
    
    # é€ä¸ªè¯»å–æ–‡ä»¶å¹¶æ”¶é›†æ•°æ®
    all_datasets = []
    
    for i, file in enumerate(existing_files):
        print(f"è¯»å–æ–‡ä»¶ {i+1}/{len(existing_files)}: {file}")
        
        try:
            ds = xr.open_dataset(file)
            
            # æ£€æŸ¥æ•°æ®ç»´åº¦
            print(f"  æ—¶é—´æ­¥: {len(ds.valid_time)}, ç½‘æ ¼: {len(ds.latitude)}Ã—{len(ds.longitude)}, å˜é‡: {len(ds.data_vars)}")
            
            # ç¡®ä¿æ•°æ®è¢«å®é™…åŠ è½½
            # é€šè¿‡è®¡ç®—ä¸€ä¸ªå˜é‡çš„å‡å€¼æ¥å¼ºåˆ¶åŠ è½½æ•°æ®
            sample_var = list(ds.data_vars.keys())[0]
            _ = ds[sample_var].mean().values  # å¼ºåˆ¶è®¡ç®—
            
            all_datasets.append(ds)
            
        except Exception as e:
            print(f"  âŒ è¯»å–æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
            continue
    
    if not all_datasets:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ•°æ®é›†")
        return None
    
    print(f"\næˆåŠŸè¯»å– {len(all_datasets)} ä¸ªæ•°æ®é›†ï¼Œå¼€å§‹åˆå¹¶...")
    
    try:
        # æ–¹æ³•1: ä½¿ç”¨concatåˆå¹¶ï¼ˆæ›´å¯é ï¼‰
        print("ä½¿ç”¨concatæ–¹æ³•åˆå¹¶...")
        merged_ds = xr.concat(all_datasets, dim='valid_time')
        
        # ç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        merged_ds = merged_ds.sortby('valid_time')
        
        # è¾“å‡ºåˆå¹¶åçš„è¯¦ç»†ä¿¡æ¯
        print(f"\nâœ… åˆå¹¶å®Œæˆ!")
        print(f"æ—¶é—´èŒƒå›´: {merged_ds.valid_time.values[0]} åˆ° {merged_ds.valid_time.values[-1]}")
        print(f"æ€»æ—¶é—´æ­¥æ•°: {len(merged_ds.valid_time)}")
        print(f"ç©ºé—´ç½‘æ ¼: {len(merged_ds.latitude)} Ã— {len(merged_ds.longitude)}")
        print(f"å˜é‡æ•°é‡: {len(merged_ds.data_vars)}")
        print(f"å˜é‡åˆ—è¡¨: {list(merged_ds.data_vars.keys())}")
        
        # è®¡ç®—é¢„æœŸçš„æ•°æ®å¤§å°
        n_times = len(merged_ds.valid_time)
        n_lats = len(merged_ds.latitude)
        n_lons = len(merged_ds.longitude)
        n_vars = len(merged_ds.data_vars)
        # å‡è®¾float32æ•°æ® (4å­—èŠ‚)
        expected_size = n_times * n_lats * n_lons * n_vars * 4 / 1024 / 1024  # MB
        
        print(f"é¢„æœŸæ•°æ®å¤§å°: ~{expected_size:.1f} MB")
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        output_file = "land_merged_2024_03-12.nc"
        
        # ä½¿ç”¨æ›´å®‰å…¨çš„ä¿å­˜é€‰é¡¹
        encoding = {var: {'dtype': 'float32', 'zlib': True} for var in merged_ds.data_vars}
        merged_ds.to_netcdf(output_file, encoding=encoding)
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤§å°
        output_size = os.path.getsize(output_file) / 1024 / 1024  # MB
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file} ({output_size:.1f} MB)")
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        verify_output_file(output_file)
        
        # å…³é—­æ‰€æœ‰æ•°æ®é›†
        for ds in all_datasets:
            ds.close()
            
        return merged_ds
        
    except Exception as e:
        print(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        # å…³é—­æ‰€æœ‰æ•°æ®é›†
        for ds in all_datasets:
            ds.close()
            
        return None

def verify_output_file(file_path):
    """
    éªŒè¯è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§
    """
    if not os.path.exists(file_path):
        print(f"âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    try:
        ds = xr.open_dataset(file_path)
        
        print(f"\nğŸ” è¾“å‡ºæ–‡ä»¶éªŒè¯:")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024 / 1024:.1f} MB")
        print(f"æ—¶é—´æ­¥æ•°: {len(ds.valid_time)}")
        print(f"ç©ºé—´ç½‘æ ¼: {len(ds.latitude)} Ã— {len(ds.longitude)}")
        print(f"å˜é‡æ•°é‡: {len(ds.data_vars)}")
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        for var_name in ds.data_vars:
            data = ds[var_name]
            print(f"  {var_name}: {data.shape}, èŒƒå›´: [{float(data.min().values):.3f}, {float(data.max().values):.3f}]")
        
        ds.close()
        print("âœ… æ–‡ä»¶éªŒè¯é€šè¿‡")
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")

def alternative_merge_method():
    """
    å¤‡é€‰åˆå¹¶æ–¹æ³•ï¼šé€ä¸ªå˜é‡åˆå¹¶
    """
    print("\nå°è¯•å¤‡é€‰åˆå¹¶æ–¹æ³•...")
    
    file_list = [
        'land_merged_2024-03.nc',
        'land_merged_2024-04.nc', 
        'land_merged_2024-05.nc',
        'land_merged_2024-06.nc',
        'land_merged_2024-07.nc',
        'land_merged_2024-08.nc',
        'land_merged_2024-09.nc',
        'land_merged_2024-10.nc',
        'land_merged_2024-11.nc',
        'land_merged_2024-12.nc'
    ]
    
    existing_files = [f for f in file_list if os.path.exists(f)]
    
    if not existing_files:
        return None
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºæ¨¡æ¿
    template = xr.open_dataset(existing_files[0])
    var_names = list(template.data_vars.keys())
    
    # ä¸ºæ¯ä¸ªå˜é‡åˆ›å»ºç©ºåˆ—è¡¨
    var_data = {var: [] for var in var_names}
    
    # æ”¶é›†æ‰€æœ‰æ•°æ®
    for file in existing_files:
        ds = xr.open_dataset(file)
        for var in var_names:
            var_data[var].append(ds[var])
        ds.close()
    
    template.close()
    
    # åˆå¹¶æ¯ä¸ªå˜é‡
    merged_vars = {}
    for var in var_names:
        merged_vars[var] = xr.concat(var_data[var], dim='valid_time')
    
    # åˆ›å»ºæ–°æ•°æ®é›†
    merged_ds = xr.Dataset(
        merged_vars,
        coords={
            'valid_time': merged_vars[var_names[0]].valid_time,
            'latitude': merged_vars[var_names[0]].latitude,
            'longitude': merged_vars[var_names[0]].longitude
        }
    )
    
    # æŒ‰æ—¶é—´æ’åº
    merged_ds = merged_ds.sortby('valid_time')
    
    # ä¿å­˜
    output_file = "land_merged_2024_03-12_alt.nc"
    merged_ds.to_netcdf(output_file)
    
    print(f"å¤‡é€‰æ–¹æ³•å®Œæˆ: {output_file}")
    return merged_ds

# ä¸»æ‰§è¡Œç¨‹åº
if __name__ == "__main__":
    print("å¼€å§‹åˆå¹¶landæ•°æ®æ–‡ä»¶ (2024å¹´3æœˆ-12æœˆ)...")
    
    # é¦–å…ˆå°è¯•ä¸»è¦æ–¹æ³•
    merged_data = merge_land_data_safe()
    
    if merged_data is None:
        print("\nä¸»è¦æ–¹æ³•å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•...")
        merged_data = alternative_merge_method()
    
    if merged_data is not None:
        print(f"\nâœ… landæ•°æ®åˆå¹¶æˆåŠŸå®Œæˆ!")
        print(f"æœ€ç»ˆæ–‡ä»¶å¤§å°: {os.path.getsize('land_merged_2024_03-12.nc') / 1024 / 1024:.1f} MB")
    else:
        print("âŒ landæ•°æ®åˆå¹¶å¤±è´¥")