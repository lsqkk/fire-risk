{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f13b383-daab-4e04-9130-77bc49b6b35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:39:46.109878Z",
     "iopub.status.busy": "2025-11-18T18:39:46.109878Z",
     "iopub.status.idle": "2025-11-18T18:39:46.165892Z",
     "shell.execute_reply": "2025-11-18T18:39:46.160950Z",
     "shell.execute_reply.started": "2025-11-18T18:39:46.109878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chene\\anaconda3\\envs\\Fire_risk_2\\lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from paddle.io import Dataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16aa3b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddle.device.set_device('gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8143e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:39:47.444532Z",
     "iopub.status.busy": "2025-11-18T18:39:47.441531Z",
     "iopub.status.idle": "2025-11-18T18:39:47.501123Z",
     "shell.execute_reply": "2025-11-18T18:39:47.495181Z",
     "shell.execute_reply.started": "2025-11-18T18:39:47.444532Z"
    }
   },
   "outputs": [],
   "source": [
    "data_paths={'land':'YOUR/PATH/land_with_precipitation_8channels.nc',\n",
    "            'pressure_levels':'YOUR/PATH/pressure_cropped_440x408.nc',\n",
    "            'label':'YOUR/PATH/fire_risk_binary_final.npy'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5fe131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FireRiskConv\n",
    "\n",
    "mymodel_conv = FireRiskConv(in_channels = 6*8+10,\n",
    "                            adapter_channels = 64,\n",
    "                            mlp_hidden_channels = 128,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef61c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_path = \"YOUR/MODEL/PATH.pdparams\"\n",
    "loaded_state_dict = paddle.load(params_path)\n",
    "\n",
    "mymodel_conv.set_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8328fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: LinearWarmup set learning rate to 0.0.\n",
      "时间戳类型: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "第一个NC文件可用变量: ['lai_lv', 'lai_hv', 'sp', 'v10', 'u10', 'skt', 'd2m', 'precipitation']\n",
      "第二个NC文件可用变量: ['v_850', 'v_500', 'u_850', 'u_500', 't_850', 't_500', 'q_850', 'q_500', 'z_850', 'z_500']\n",
      "=== 时间信息 ===\n",
      "时间范围: 2024-03-01 00:00:00 到 2025-02-28 00:00:00\n",
      "总时间步数: 365\n",
      "可用样本数: 359\n",
      "Epoch 1: LinearWarmup set learning rate to 0.001392757660167131.\n",
      "Epoch: 0, Batch: 0, Loss: 99.771614\n",
      "Epoch 2: LinearWarmup set learning rate to 0.002785515320334262.\n",
      "Epoch: 0, Batch: 1, Loss: 0.000000\n",
      "Epoch 3: LinearWarmup set learning rate to 0.004178272980501393.\n",
      "Epoch: 0, Batch: 2, Loss: 98.994537\n",
      "Epoch 4: LinearWarmup set learning rate to 0.005571030640668524.\n",
      "Epoch: 0, Batch: 3, Loss: 99.771614\n",
      "Epoch 5: LinearWarmup set learning rate to 0.006963788300835654.\n",
      "Epoch: 0, Batch: 4, Loss: 99.771614\n",
      "Epoch 6: LinearWarmup set learning rate to 0.008356545961002786.\n",
      "Epoch: 0, Batch: 5, Loss: 99.771614\n",
      "Epoch 7: LinearWarmup set learning rate to 0.009749303621169917.\n",
      "Epoch: 0, Batch: 6, Loss: 99.727051\n",
      "Epoch 8: LinearWarmup set learning rate to 0.011142061281337047.\n",
      "Epoch: 0, Batch: 7, Loss: 99.771614\n",
      "Epoch 9: LinearWarmup set learning rate to 0.012534818941504178.\n",
      "Epoch: 0, Batch: 8, Loss: 99.769943\n",
      "Epoch 10: LinearWarmup set learning rate to 0.013927576601671309.\n",
      "Epoch: 0, Batch: 9, Loss: 99.747101\n",
      "Epoch 11: LinearWarmup set learning rate to 0.01532033426183844.\n",
      "Epoch: 0, Batch: 10, Loss: 99.769943\n",
      "Epoch 12: LinearWarmup set learning rate to 0.016713091922005572.\n",
      "Epoch: 0, Batch: 11, Loss: 99.771614\n",
      "Epoch 13: LinearWarmup set learning rate to 0.018105849582172703.\n",
      "Epoch: 0, Batch: 12, Loss: 99.766602\n",
      "Epoch 14: LinearWarmup set learning rate to 0.019498607242339833.\n",
      "Epoch: 0, Batch: 13, Loss: 99.771614\n",
      "Epoch 15: LinearWarmup set learning rate to 0.020891364902506964.\n",
      "Epoch: 0, Batch: 14, Loss: 99.771614\n",
      "Epoch 16: LinearWarmup set learning rate to 0.022284122562674095.\n",
      "Epoch: 0, Batch: 15, Loss: 99.771614\n",
      "Epoch 17: LinearWarmup set learning rate to 0.023676880222841225.\n",
      "Epoch: 0, Batch: 16, Loss: 99.769943\n",
      "Epoch 18: LinearWarmup set learning rate to 0.025069637883008356.\n",
      "Epoch: 0, Batch: 17, Loss: 99.719810\n",
      "Epoch 19: LinearWarmup set learning rate to 0.026462395543175487.\n",
      "Epoch: 0, Batch: 18, Loss: 99.771614\n",
      "Epoch 20: LinearWarmup set learning rate to 0.027855153203342618.\n",
      "Epoch: 0, Batch: 19, Loss: 99.771057\n",
      "Epoch 21: LinearWarmup set learning rate to 0.02924791086350975.\n",
      "Epoch: 0, Batch: 20, Loss: 99.770500\n",
      "Epoch 22: LinearWarmup set learning rate to 0.03064066852367688.\n",
      "Epoch: 0, Batch: 21, Loss: 99.771614\n",
      "Epoch 23: LinearWarmup set learning rate to 0.03203342618384401.\n",
      "Epoch: 0, Batch: 22, Loss: 99.631241\n",
      "Epoch 24: LinearWarmup set learning rate to 0.033426183844011144.\n",
      "Epoch: 0, Batch: 23, Loss: 99.525398\n",
      "Epoch 25: LinearWarmup set learning rate to 0.034818941504178275.\n",
      "Epoch: 0, Batch: 24, Loss: 99.769386\n",
      "Epoch 26: LinearWarmup set learning rate to 0.036211699164345405.\n",
      "Epoch: 0, Batch: 25, Loss: 99.729279\n",
      "Epoch 27: LinearWarmup set learning rate to 0.037604456824512536.\n",
      "Epoch: 0, Batch: 26, Loss: 99.771614\n",
      "Epoch 28: LinearWarmup set learning rate to 0.03899721448467967.\n",
      "Epoch: 0, Batch: 27, Loss: 99.771614\n",
      "Epoch 29: LinearWarmup set learning rate to 0.0403899721448468.\n",
      "Epoch: 0, Batch: 28, Loss: 99.771614\n",
      "Epoch 30: LinearWarmup set learning rate to 0.04178272980501393.\n",
      "Epoch: 0, Batch: 29, Loss: 99.771614\n",
      "Epoch 31: LinearWarmup set learning rate to 0.04317548746518106.\n",
      "Epoch: 0, Batch: 30, Loss: 99.771057\n",
      "Epoch 32: LinearWarmup set learning rate to 0.04456824512534819.\n",
      "Epoch: 0, Batch: 31, Loss: 99.771614\n",
      "Epoch 33: LinearWarmup set learning rate to 0.04596100278551532.\n",
      "Epoch: 0, Batch: 32, Loss: 99.771614\n",
      "Epoch 34: LinearWarmup set learning rate to 0.04735376044568245.\n",
      "Epoch: 0, Batch: 33, Loss: 99.771614\n",
      "Epoch 35: LinearWarmup set learning rate to 0.04874651810584958.\n",
      "Epoch: 0, Batch: 34, Loss: 0.000000\n",
      "Epoch 36: LinearWarmup set learning rate to 0.05013927576601671.\n",
      "Epoch: 0, Batch: 35, Loss: 99.771614\n",
      "Epoch 37: LinearWarmup set learning rate to 0.05153203342618384.\n",
      "Epoch: 0, Batch: 36, Loss: 99.769943\n",
      "Epoch 38: LinearWarmup set learning rate to 0.052924791086350974.\n",
      "Epoch: 0, Batch: 37, Loss: 99.771614\n",
      "Epoch 39: LinearWarmup set learning rate to 0.054317548746518104.\n",
      "Epoch: 0, Batch: 38, Loss: 99.771614\n",
      "Epoch 40: LinearWarmup set learning rate to 0.055710306406685235.\n",
      "Epoch: 0, Batch: 39, Loss: 99.771614\n",
      "Epoch 41: LinearWarmup set learning rate to 0.057103064066852366.\n",
      "Epoch: 0, Batch: 40, Loss: 99.771614\n",
      "Epoch 42: LinearWarmup set learning rate to 0.0584958217270195.\n",
      "Epoch: 0, Batch: 41, Loss: 99.769386\n",
      "Epoch 43: LinearWarmup set learning rate to 0.05988857938718663.\n",
      "Epoch: 0, Batch: 42, Loss: 99.771614\n",
      "Epoch 44: LinearWarmup set learning rate to 0.06128133704735376.\n",
      "Epoch: 0, Batch: 43, Loss: 99.770500\n",
      "Epoch 45: LinearWarmup set learning rate to 0.06267409470752089.\n",
      "Epoch: 0, Batch: 44, Loss: 99.771614\n",
      "Epoch 46: LinearWarmup set learning rate to 0.06406685236768803.\n",
      "Epoch: 0, Batch: 45, Loss: 99.771614\n",
      "Epoch 47: LinearWarmup set learning rate to 0.06545961002785515.\n",
      "Epoch: 0, Batch: 46, Loss: 99.771614\n",
      "Epoch 48: LinearWarmup set learning rate to 0.06685236768802229.\n",
      "Epoch: 0, Batch: 47, Loss: 99.771614\n",
      "Epoch 49: LinearWarmup set learning rate to 0.06824512534818941.\n",
      "Epoch: 0, Batch: 48, Loss: 99.771614\n",
      "Epoch 50: LinearWarmup set learning rate to 0.06963788300835655.\n",
      "Epoch: 0, Batch: 49, Loss: 99.771614\n",
      "Epoch 51: LinearWarmup set learning rate to 0.07103064066852367.\n",
      "Epoch: 0, Batch: 50, Loss: 99.771614\n",
      "Epoch 52: LinearWarmup set learning rate to 0.07242339832869081.\n",
      "Epoch: 0, Batch: 51, Loss: 99.771614\n",
      "Epoch 53: LinearWarmup set learning rate to 0.07381615598885793.\n",
      "Epoch: 0, Batch: 52, Loss: 99.771057\n",
      "Epoch 54: LinearWarmup set learning rate to 0.07520891364902507.\n",
      "Epoch: 0, Batch: 53, Loss: 99.771614\n",
      "Epoch 55: LinearWarmup set learning rate to 0.0766016713091922.\n",
      "Epoch: 0, Batch: 54, Loss: 99.771614\n",
      "Epoch 56: LinearWarmup set learning rate to 0.07799442896935933.\n",
      "Epoch: 0, Batch: 55, Loss: 99.771614\n",
      "Epoch 57: LinearWarmup set learning rate to 0.07938718662952646.\n",
      "Epoch: 0, Batch: 56, Loss: 99.766045\n",
      "Epoch 58: LinearWarmup set learning rate to 0.0807799442896936.\n",
      "Epoch: 0, Batch: 57, Loss: 99.766602\n",
      "Epoch 59: LinearWarmup set learning rate to 0.08217270194986072.\n",
      "Epoch: 0, Batch: 58, Loss: 99.771614\n",
      "Epoch 60: LinearWarmup set learning rate to 0.08356545961002786.\n",
      "Epoch: 0, Batch: 59, Loss: 99.771614\n",
      "Epoch 61: LinearWarmup set learning rate to 0.08495821727019498.\n",
      "Epoch: 0, Batch: 60, Loss: 99.771614\n",
      "Epoch 62: LinearWarmup set learning rate to 0.08635097493036212.\n",
      "Epoch: 0, Batch: 61, Loss: 99.770500\n",
      "Epoch 63: LinearWarmup set learning rate to 0.08774373259052924.\n",
      "Epoch: 0, Batch: 62, Loss: 99.771614\n",
      "Epoch 64: LinearWarmup set learning rate to 0.08913649025069638.\n",
      "Epoch: 0, Batch: 63, Loss: 99.771057\n",
      "Epoch 65: LinearWarmup set learning rate to 0.0905292479108635.\n",
      "Epoch: 0, Batch: 64, Loss: 99.771614\n",
      "Epoch 66: LinearWarmup set learning rate to 0.09192200557103064.\n",
      "Epoch: 0, Batch: 65, Loss: 99.747101\n",
      "Epoch 67: LinearWarmup set learning rate to 0.09331476323119778.\n",
      "Epoch: 0, Batch: 66, Loss: 99.771614\n",
      "Epoch 68: LinearWarmup set learning rate to 0.0947075208913649.\n",
      "Epoch: 0, Batch: 67, Loss: 99.771614\n",
      "Epoch 69: LinearWarmup set learning rate to 0.09610027855153204.\n",
      "Epoch: 0, Batch: 68, Loss: 0.000000\n",
      "Epoch 70: LinearWarmup set learning rate to 0.09749303621169916.\n",
      "Epoch: 0, Batch: 69, Loss: 99.770500\n",
      "Epoch 71: LinearWarmup set learning rate to 0.0988857938718663.\n",
      "Epoch: 0, Batch: 70, Loss: 99.771614\n",
      "Epoch 72: LinearWarmup set learning rate to 0.10027855153203342.\n",
      "Epoch: 0, Batch: 71, Loss: 99.771614\n",
      "Epoch 73: LinearWarmup set learning rate to 0.10167130919220056.\n",
      "Epoch: 0, Batch: 72, Loss: 0.000000\n",
      "Epoch 74: LinearWarmup set learning rate to 0.10306406685236769.\n",
      "Epoch: 0, Batch: 73, Loss: 99.771057\n",
      "Epoch 75: LinearWarmup set learning rate to 0.10445682451253482.\n",
      "Epoch: 0, Batch: 74, Loss: 99.771057\n",
      "Epoch 76: LinearWarmup set learning rate to 0.10584958217270195.\n",
      "Epoch: 0, Batch: 75, Loss: 99.771614\n",
      "Epoch 77: LinearWarmup set learning rate to 0.10724233983286909.\n",
      "Epoch: 0, Batch: 76, Loss: 99.771614\n",
      "Epoch 78: LinearWarmup set learning rate to 0.10863509749303621.\n",
      "Epoch: 0, Batch: 77, Loss: 99.771614\n",
      "Epoch 79: LinearWarmup set learning rate to 0.11002785515320335.\n",
      "Epoch: 0, Batch: 78, Loss: 99.771614\n",
      "Epoch 80: LinearWarmup set learning rate to 0.11142061281337047.\n",
      "Epoch: 0, Batch: 79, Loss: 99.771614\n",
      "Epoch 81: LinearWarmup set learning rate to 0.11281337047353761.\n",
      "Epoch: 0, Batch: 80, Loss: 99.771614\n",
      "Epoch 82: LinearWarmup set learning rate to 0.11420612813370473.\n",
      "Epoch: 0, Batch: 81, Loss: 99.771614\n",
      "Epoch 83: LinearWarmup set learning rate to 0.11559888579387187.\n",
      "Epoch: 0, Batch: 82, Loss: 0.000000\n",
      "Epoch 84: LinearWarmup set learning rate to 0.116991643454039.\n",
      "Epoch: 0, Batch: 83, Loss: 99.771614\n",
      "Epoch 85: LinearWarmup set learning rate to 0.11838440111420613.\n",
      "Epoch: 0, Batch: 84, Loss: 99.329880\n",
      "Epoch 86: LinearWarmup set learning rate to 0.11977715877437325.\n",
      "Epoch: 0, Batch: 85, Loss: 99.757133\n",
      "Epoch 87: LinearWarmup set learning rate to 0.12116991643454039.\n",
      "Epoch: 0, Batch: 86, Loss: 99.771614\n",
      "Epoch 88: LinearWarmup set learning rate to 0.12256267409470752.\n",
      "Epoch: 0, Batch: 87, Loss: 99.771614\n",
      "Epoch 89: LinearWarmup set learning rate to 0.12395543175487465.\n",
      "Epoch: 0, Batch: 88, Loss: 0.000000\n",
      "Epoch 90: LinearWarmup set learning rate to 0.12534818941504178.\n",
      "Epoch: 0, Batch: 89, Loss: 99.078651\n",
      "Epoch 91: LinearWarmup set learning rate to 0.12674094707520892.\n",
      "Epoch: 0, Batch: 90, Loss: 99.770500\n",
      "Epoch 92: LinearWarmup set learning rate to 0.12813370473537605.\n",
      "Epoch: 0, Batch: 91, Loss: 99.768829\n",
      "Epoch 93: LinearWarmup set learning rate to 0.12952646239554316.\n",
      "Epoch: 0, Batch: 92, Loss: 99.581665\n",
      "Epoch 94: LinearWarmup set learning rate to 0.1309192200557103.\n",
      "Epoch: 0, Batch: 93, Loss: 99.768829\n",
      "Epoch 95: LinearWarmup set learning rate to 0.13231197771587744.\n",
      "Epoch: 0, Batch: 94, Loss: 99.771057\n",
      "Epoch 96: LinearWarmup set learning rate to 0.13370473537604458.\n",
      "Epoch: 0, Batch: 95, Loss: 99.591690\n",
      "Epoch 97: LinearWarmup set learning rate to 0.13509749303621169.\n",
      "Epoch: 0, Batch: 96, Loss: 99.771614\n",
      "Epoch 98: LinearWarmup set learning rate to 0.13649025069637882.\n",
      "Epoch: 0, Batch: 97, Loss: 99.771614\n",
      "Epoch 99: LinearWarmup set learning rate to 0.13788300835654596.\n",
      "Epoch: 0, Batch: 98, Loss: 99.771614\n",
      "Epoch 100: LinearWarmup set learning rate to 0.1392757660167131.\n",
      "Epoch: 0, Batch: 99, Loss: 99.771057\n",
      "Epoch 101: LinearWarmup set learning rate to 0.14066852367688024.\n",
      "Epoch: 0, Batch: 100, Loss: 0.000000\n",
      "Epoch 102: LinearWarmup set learning rate to 0.14206128133704735.\n",
      "Epoch: 0, Batch: 101, Loss: 99.771614\n",
      "Epoch 103: LinearWarmup set learning rate to 0.14345403899721448.\n",
      "Epoch: 0, Batch: 102, Loss: 99.771614\n",
      "Epoch 104: LinearWarmup set learning rate to 0.14484679665738162.\n",
      "Epoch: 0, Batch: 103, Loss: 99.771614\n",
      "Epoch 105: LinearWarmup set learning rate to 0.14623955431754876.\n",
      "Epoch: 0, Batch: 104, Loss: 99.771614\n",
      "Epoch 106: LinearWarmup set learning rate to 0.14763231197771587.\n",
      "Epoch: 0, Batch: 105, Loss: 99.771614\n",
      "Epoch 107: LinearWarmup set learning rate to 0.149025069637883.\n",
      "Epoch: 0, Batch: 106, Loss: 99.770500\n",
      "Epoch 108: LinearWarmup set learning rate to 0.15041782729805014.\n",
      "Epoch: 0, Batch: 107, Loss: 99.684715\n",
      "Epoch 109: LinearWarmup set learning rate to 0.15181058495821728.\n",
      "Epoch: 0, Batch: 108, Loss: 99.771614\n",
      "Epoch 110: LinearWarmup set learning rate to 0.1532033426183844.\n",
      "Epoch: 0, Batch: 109, Loss: 99.771614\n",
      "Epoch 111: LinearWarmup set learning rate to 0.15459610027855153.\n",
      "Epoch: 0, Batch: 110, Loss: 99.771614\n",
      "Epoch 112: LinearWarmup set learning rate to 0.15598885793871867.\n",
      "Epoch: 0, Batch: 111, Loss: 99.764374\n",
      "Epoch 113: LinearWarmup set learning rate to 0.1573816155988858.\n",
      "Epoch: 0, Batch: 112, Loss: 99.771614\n",
      "Epoch 114: LinearWarmup set learning rate to 0.15877437325905291.\n",
      "Epoch: 0, Batch: 113, Loss: 99.759918\n",
      "Epoch 115: LinearWarmup set learning rate to 0.16016713091922005.\n",
      "Epoch: 0, Batch: 114, Loss: 99.770500\n",
      "Epoch 116: LinearWarmup set learning rate to 0.1615598885793872.\n",
      "Epoch: 0, Batch: 115, Loss: 99.771614\n",
      "Epoch 117: LinearWarmup set learning rate to 0.16295264623955433.\n",
      "Epoch: 0, Batch: 116, Loss: 0.000000\n",
      "Epoch 118: LinearWarmup set learning rate to 0.16434540389972144.\n",
      "Epoch: 0, Batch: 117, Loss: 99.771614\n",
      "Epoch 119: LinearWarmup set learning rate to 0.16573816155988857.\n",
      "Epoch: 0, Batch: 118, Loss: 99.771614\n",
      "Epoch 120: LinearWarmup set learning rate to 0.1671309192200557.\n",
      "Epoch: 0, Batch: 119, Loss: 99.771614\n",
      "Epoch 121: LinearWarmup set learning rate to 0.16852367688022285.\n",
      "Epoch: 0, Batch: 120, Loss: 99.769943\n",
      "Epoch 122: LinearWarmup set learning rate to 0.16991643454038996.\n",
      "Epoch: 0, Batch: 121, Loss: 0.000000\n",
      "Epoch 123: LinearWarmup set learning rate to 0.1713091922005571.\n",
      "Epoch: 0, Batch: 122, Loss: 99.643494\n",
      "Epoch 124: LinearWarmup set learning rate to 0.17270194986072424.\n",
      "Epoch: 0, Batch: 123, Loss: 99.771614\n",
      "Epoch 125: LinearWarmup set learning rate to 0.17409470752089137.\n",
      "Epoch: 0, Batch: 124, Loss: 99.771057\n",
      "Epoch 126: LinearWarmup set learning rate to 0.17548746518105848.\n",
      "Epoch: 0, Batch: 125, Loss: 99.660202\n",
      "Epoch 127: LinearWarmup set learning rate to 0.17688022284122562.\n",
      "Epoch: 0, Batch: 126, Loss: 99.771614\n",
      "Epoch 128: LinearWarmup set learning rate to 0.17827298050139276.\n",
      "Epoch: 0, Batch: 127, Loss: 99.771057\n",
      "Epoch 129: LinearWarmup set learning rate to 0.1796657381615599.\n",
      "Epoch: 0, Batch: 128, Loss: 99.771614\n",
      "Epoch 130: LinearWarmup set learning rate to 0.181058495821727.\n",
      "Epoch: 0, Batch: 129, Loss: 99.771614\n",
      "Epoch 131: LinearWarmup set learning rate to 0.18245125348189414.\n",
      "Epoch: 0, Batch: 130, Loss: 99.640152\n",
      "Epoch 132: LinearWarmup set learning rate to 0.18384401114206128.\n",
      "Epoch: 0, Batch: 131, Loss: 0.000000\n",
      "Epoch 133: LinearWarmup set learning rate to 0.18523676880222842.\n",
      "Epoch: 0, Batch: 132, Loss: 99.770500\n",
      "Epoch 134: LinearWarmup set learning rate to 0.18662952646239556.\n",
      "Epoch: 0, Batch: 133, Loss: 99.768272\n",
      "Epoch 135: LinearWarmup set learning rate to 0.18802228412256267.\n",
      "Epoch: 0, Batch: 134, Loss: 99.771614\n",
      "Epoch 136: LinearWarmup set learning rate to 0.1894150417827298.\n",
      "Epoch: 0, Batch: 135, Loss: 99.771614\n",
      "Epoch 137: LinearWarmup set learning rate to 0.19080779944289694.\n",
      "Epoch: 0, Batch: 136, Loss: 99.771614\n",
      "Epoch 138: LinearWarmup set learning rate to 0.19220055710306408.\n",
      "Epoch: 0, Batch: 137, Loss: 99.771614\n",
      "Epoch 139: LinearWarmup set learning rate to 0.1935933147632312.\n",
      "Epoch: 0, Batch: 138, Loss: 98.885918\n",
      "Epoch 140: LinearWarmup set learning rate to 0.19498607242339833.\n",
      "Epoch: 0, Batch: 139, Loss: 99.771614\n",
      "Epoch 141: LinearWarmup set learning rate to 0.19637883008356546.\n",
      "Epoch: 0, Batch: 140, Loss: 99.771057\n",
      "Epoch 142: LinearWarmup set learning rate to 0.1977715877437326.\n",
      "Epoch: 0, Batch: 141, Loss: 0.000000\n",
      "Epoch 143: LinearWarmup set learning rate to 0.1991643454038997.\n",
      "Epoch: 0, Batch: 142, Loss: 99.771614\n",
      "Epoch 144: LinearWarmup set learning rate to 0.20055710306406685.\n",
      "Epoch: 0, Batch: 143, Loss: 99.771614\n",
      "Epoch 145: LinearWarmup set learning rate to 0.201949860724234.\n",
      "Epoch: 0, Batch: 144, Loss: 99.760475\n",
      "Epoch 146: LinearWarmup set learning rate to 0.20334261838440112.\n",
      "Epoch: 0, Batch: 145, Loss: 99.769943\n",
      "Epoch 147: LinearWarmup set learning rate to 0.20473537604456823.\n",
      "Epoch: 0, Batch: 146, Loss: 99.771614\n",
      "Epoch 148: LinearWarmup set learning rate to 0.20612813370473537.\n",
      "Epoch: 0, Batch: 147, Loss: 99.502563\n",
      "Epoch 149: LinearWarmup set learning rate to 0.2075208913649025.\n",
      "Epoch: 0, Batch: 148, Loss: 99.771614\n",
      "Epoch 150: LinearWarmup set learning rate to 0.20891364902506965.\n",
      "Epoch: 0, Batch: 149, Loss: 99.770500\n",
      "Epoch 151: LinearWarmup set learning rate to 0.21030640668523676.\n",
      "Epoch: 0, Batch: 150, Loss: 99.771614\n",
      "Epoch 152: LinearWarmup set learning rate to 0.2116991643454039.\n",
      "Epoch: 0, Batch: 151, Loss: 99.470253\n",
      "Epoch 153: LinearWarmup set learning rate to 0.21309192200557103.\n",
      "Epoch: 0, Batch: 152, Loss: 99.771614\n",
      "Epoch 154: LinearWarmup set learning rate to 0.21448467966573817.\n",
      "Epoch: 0, Batch: 153, Loss: 99.771057\n",
      "Epoch 155: LinearWarmup set learning rate to 0.21587743732590528.\n",
      "Epoch: 0, Batch: 154, Loss: 99.581108\n",
      "Epoch 156: LinearWarmup set learning rate to 0.21727019498607242.\n",
      "Epoch: 0, Batch: 155, Loss: 99.771614\n",
      "Epoch 157: LinearWarmup set learning rate to 0.21866295264623956.\n",
      "Epoch: 0, Batch: 156, Loss: 99.767715\n",
      "Epoch 158: LinearWarmup set learning rate to 0.2200557103064067.\n",
      "Epoch: 0, Batch: 157, Loss: 0.000000\n",
      "Epoch 159: LinearWarmup set learning rate to 0.2214484679665738.\n",
      "Epoch: 0, Batch: 158, Loss: 99.718697\n",
      "Epoch 160: LinearWarmup set learning rate to 0.22284122562674094.\n",
      "Epoch: 0, Batch: 159, Loss: 99.770500\n",
      "Epoch 161: LinearWarmup set learning rate to 0.22423398328690808.\n",
      "Epoch: 0, Batch: 160, Loss: 99.771614\n",
      "Epoch 162: LinearWarmup set learning rate to 0.22562674094707522.\n",
      "Epoch: 0, Batch: 161, Loss: 99.762146\n",
      "Epoch 163: LinearWarmup set learning rate to 0.22701949860724233.\n",
      "Epoch: 0, Batch: 162, Loss: 99.664658\n",
      "Epoch 164: LinearWarmup set learning rate to 0.22841225626740946.\n",
      "Epoch: 0, Batch: 163, Loss: 0.000000\n",
      "Epoch 165: LinearWarmup set learning rate to 0.2298050139275766.\n",
      "Epoch: 0, Batch: 164, Loss: 99.771088\n",
      "Epoch 166: LinearWarmup set learning rate to 0.23119777158774374.\n",
      "Epoch: 0, Batch: 165, Loss: 0.000000\n",
      "Epoch 167: LinearWarmup set learning rate to 0.23259052924791088.\n",
      "Epoch: 0, Batch: 166, Loss: 0.962567\n",
      "Epoch 168: LinearWarmup set learning rate to 0.233983286908078.\n",
      "Epoch: 0, Batch: 167, Loss: 0.016154\n",
      "Epoch 169: LinearWarmup set learning rate to 0.23537604456824512.\n",
      "Epoch: 0, Batch: 168, Loss: 0.000000\n",
      "Epoch 170: LinearWarmup set learning rate to 0.23676880222841226.\n",
      "Epoch: 0, Batch: 169, Loss: 0.000000\n",
      "Epoch 171: LinearWarmup set learning rate to 0.2381615598885794.\n",
      "Epoch: 0, Batch: 170, Loss: 0.000000\n",
      "Epoch 172: LinearWarmup set learning rate to 0.2395543175487465.\n",
      "Epoch: 0, Batch: 171, Loss: 0.038993\n",
      "Epoch 173: LinearWarmup set learning rate to 0.24094707520891365.\n",
      "Epoch: 0, Batch: 172, Loss: 0.000000\n",
      "Epoch 174: LinearWarmup set learning rate to 0.24233983286908078.\n",
      "Epoch: 0, Batch: 173, Loss: 0.000000\n",
      "Epoch 175: LinearWarmup set learning rate to 0.24373259052924792.\n",
      "Epoch: 0, Batch: 174, Loss: 0.000000\n",
      "Epoch 176: LinearWarmup set learning rate to 0.24512534818941503.\n",
      "Epoch: 0, Batch: 175, Loss: 0.000000\n",
      "Epoch 177: LinearWarmup set learning rate to 0.24651810584958217.\n",
      "Epoch: 0, Batch: 176, Loss: 0.000000\n",
      "Epoch 178: LinearWarmup set learning rate to 0.2479108635097493.\n",
      "Epoch: 0, Batch: 177, Loss: 0.000000\n",
      "Epoch 179: LinearWarmup set learning rate to 0.24930362116991645.\n",
      "Epoch: 0, Batch: 178, Loss: 0.000000\n",
      "Epoch 180: LinearWarmup set learning rate to 0.25069637883008355.\n",
      "Epoch: 0, Batch: 179, Loss: 0.090241\n",
      "Epoch 181: LinearWarmup set learning rate to 0.2520891364902507.\n",
      "Epoch: 0, Batch: 180, Loss: 0.001671\n",
      "Epoch 182: LinearWarmup set learning rate to 0.25348189415041783.\n",
      "Epoch: 0, Batch: 181, Loss: 0.000000\n",
      "Epoch 183: LinearWarmup set learning rate to 0.25487465181058494.\n",
      "Epoch: 0, Batch: 182, Loss: 0.000000\n",
      "Epoch 184: LinearWarmup set learning rate to 0.2562674094707521.\n",
      "Epoch: 0, Batch: 183, Loss: 0.000000\n",
      "Epoch 185: LinearWarmup set learning rate to 0.2576601671309192.\n",
      "Epoch: 0, Batch: 184, Loss: 0.000000\n",
      "Epoch 186: LinearWarmup set learning rate to 0.2590529247910863.\n",
      "Epoch: 0, Batch: 185, Loss: 0.000557\n",
      "Epoch 187: LinearWarmup set learning rate to 0.2604456824512535.\n",
      "Epoch: 0, Batch: 186, Loss: 0.011698\n",
      "Epoch 188: LinearWarmup set learning rate to 0.2618384401114206.\n",
      "Epoch: 0, Batch: 187, Loss: 0.000000\n",
      "Epoch 189: LinearWarmup set learning rate to 0.26323119777158777.\n",
      "Epoch: 0, Batch: 188, Loss: 0.009470\n",
      "Epoch 190: LinearWarmup set learning rate to 0.2646239554317549.\n",
      "Epoch: 0, Batch: 189, Loss: 0.000000\n",
      "Epoch 191: LinearWarmup set learning rate to 0.266016713091922.\n",
      "Epoch: 0, Batch: 190, Loss: 0.000557\n",
      "Epoch 192: LinearWarmup set learning rate to 0.26740947075208915.\n",
      "Epoch: 0, Batch: 191, Loss: 0.000000\n",
      "Epoch 193: LinearWarmup set learning rate to 0.26880222841225626.\n",
      "Epoch: 0, Batch: 192, Loss: 0.001114\n",
      "Epoch 194: LinearWarmup set learning rate to 0.27019498607242337.\n",
      "Epoch: 0, Batch: 193, Loss: 0.000000\n",
      "Epoch 195: LinearWarmup set learning rate to 0.27158774373259054.\n",
      "Epoch: 0, Batch: 194, Loss: 0.414439\n",
      "Epoch 196: LinearWarmup set learning rate to 0.27298050139275765.\n",
      "Epoch: 0, Batch: 195, Loss: 0.001114\n",
      "Epoch 197: LinearWarmup set learning rate to 0.2743732590529248.\n",
      "Epoch: 0, Batch: 196, Loss: 0.000000\n",
      "Epoch 198: LinearWarmup set learning rate to 0.2757660167130919.\n",
      "Epoch: 0, Batch: 197, Loss: 0.169898\n",
      "Epoch 199: LinearWarmup set learning rate to 0.27715877437325903.\n",
      "Epoch: 0, Batch: 198, Loss: 0.000000\n",
      "Epoch 200: LinearWarmup set learning rate to 0.2785515320334262.\n",
      "Epoch: 0, Batch: 199, Loss: 0.000000\n",
      "Epoch 201: LinearWarmup set learning rate to 0.2799442896935933.\n",
      "Epoch: 0, Batch: 200, Loss: 0.000557\n",
      "Epoch 202: LinearWarmup set learning rate to 0.28133704735376047.\n",
      "Epoch: 0, Batch: 201, Loss: 0.001671\n",
      "Epoch 203: LinearWarmup set learning rate to 0.2827298050139276.\n",
      "Epoch: 0, Batch: 202, Loss: 0.000000\n",
      "Epoch 204: LinearWarmup set learning rate to 0.2841225626740947.\n",
      "Epoch: 0, Batch: 203, Loss: 0.000000\n",
      "Epoch 205: LinearWarmup set learning rate to 0.28551532033426186.\n",
      "Epoch: 0, Batch: 204, Loss: 0.000000\n",
      "Epoch 206: LinearWarmup set learning rate to 0.28690807799442897.\n",
      "Epoch: 0, Batch: 205, Loss: 0.001114\n",
      "Epoch 207: LinearWarmup set learning rate to 0.2883008356545961.\n",
      "Epoch: 0, Batch: 206, Loss: 0.000000\n",
      "Epoch 208: LinearWarmup set learning rate to 0.28969359331476324.\n",
      "Epoch: 0, Batch: 207, Loss: 0.000000\n",
      "Epoch 209: LinearWarmup set learning rate to 0.29108635097493035.\n",
      "Epoch: 0, Batch: 208, Loss: 0.000000\n",
      "Epoch 210: LinearWarmup set learning rate to 0.2924791086350975.\n",
      "Epoch: 0, Batch: 209, Loss: 0.000000\n",
      "Epoch 211: LinearWarmup set learning rate to 0.2938718662952646.\n",
      "Epoch: 0, Batch: 210, Loss: 0.000000\n",
      "Epoch 212: LinearWarmup set learning rate to 0.29526462395543174.\n",
      "Epoch: 0, Batch: 211, Loss: 0.000557\n",
      "Epoch 213: LinearWarmup set learning rate to 0.2966573816155989.\n",
      "Epoch: 0, Batch: 212, Loss: 0.208333\n",
      "Epoch 214: LinearWarmup set learning rate to 0.298050139275766.\n",
      "Epoch: 0, Batch: 213, Loss: 0.000000\n",
      "Epoch 215: LinearWarmup set learning rate to 0.2994428969359331.\n",
      "Epoch: 0, Batch: 214, Loss: 0.000000\n",
      "Epoch 216: LinearWarmup set learning rate to 0.3008356545961003.\n",
      "Epoch: 0, Batch: 215, Loss: 0.003342\n",
      "Epoch 217: LinearWarmup set learning rate to 0.3022284122562674.\n",
      "Epoch: 0, Batch: 216, Loss: 0.000000\n",
      "Epoch 218: LinearWarmup set learning rate to 0.30362116991643456.\n",
      "Epoch: 0, Batch: 217, Loss: 0.000000\n",
      "Epoch 219: LinearWarmup set learning rate to 0.3050139275766017.\n",
      "Epoch: 0, Batch: 218, Loss: 0.044006\n",
      "Epoch 220: LinearWarmup set learning rate to 0.3064066852367688.\n",
      "Epoch: 0, Batch: 219, Loss: 0.000000\n",
      "Epoch 221: LinearWarmup set learning rate to 0.30779944289693595.\n",
      "Epoch: 0, Batch: 220, Loss: 0.000557\n",
      "Epoch 222: LinearWarmup set learning rate to 0.30919220055710306.\n",
      "Epoch: 0, Batch: 221, Loss: 0.000000\n",
      "Epoch 223: LinearWarmup set learning rate to 0.31058495821727017.\n",
      "Epoch: 0, Batch: 222, Loss: 0.108623\n",
      "Epoch 224: LinearWarmup set learning rate to 0.31197771587743733.\n",
      "Epoch: 0, Batch: 223, Loss: 0.000000\n",
      "Epoch 225: LinearWarmup set learning rate to 0.31337047353760444.\n",
      "Epoch: 0, Batch: 224, Loss: 0.000000\n",
      "Epoch 226: LinearWarmup set learning rate to 0.3147632311977716.\n",
      "Epoch: 0, Batch: 225, Loss: 0.000000\n",
      "Epoch 227: LinearWarmup set learning rate to 0.3161559888579387.\n",
      "Epoch: 0, Batch: 226, Loss: 0.000000\n",
      "Epoch 228: LinearWarmup set learning rate to 0.31754874651810583.\n",
      "Epoch: 0, Batch: 227, Loss: 0.000000\n",
      "Epoch 229: LinearWarmup set learning rate to 0.318941504178273.\n",
      "Epoch: 0, Batch: 228, Loss: 0.067402\n",
      "Epoch 230: LinearWarmup set learning rate to 0.3203342618384401.\n",
      "Epoch: 0, Batch: 229, Loss: 0.000000\n",
      "Epoch 231: LinearWarmup set learning rate to 0.32172701949860727.\n",
      "Epoch: 0, Batch: 230, Loss: 0.000000\n",
      "Epoch 232: LinearWarmup set learning rate to 0.3231197771587744.\n",
      "Epoch: 0, Batch: 231, Loss: 0.000000\n",
      "Epoch 233: LinearWarmup set learning rate to 0.3245125348189415.\n",
      "Epoch: 0, Batch: 232, Loss: 0.000000\n",
      "Epoch 234: LinearWarmup set learning rate to 0.32590529247910865.\n",
      "Epoch: 0, Batch: 233, Loss: 0.000000\n",
      "Epoch 235: LinearWarmup set learning rate to 0.32729805013927576.\n",
      "Epoch: 0, Batch: 234, Loss: 0.004456\n",
      "Epoch 236: LinearWarmup set learning rate to 0.3286908077994429.\n",
      "Epoch: 0, Batch: 235, Loss: 0.002228\n",
      "Epoch 237: LinearWarmup set learning rate to 0.33008356545961004.\n",
      "Epoch: 0, Batch: 236, Loss: 0.000557\n",
      "Epoch 238: LinearWarmup set learning rate to 0.33147632311977715.\n",
      "Epoch: 0, Batch: 237, Loss: 0.001114\n",
      "Epoch 239: LinearWarmup set learning rate to 0.3328690807799443.\n",
      "Epoch: 0, Batch: 238, Loss: 0.000000\n",
      "Epoch 240: LinearWarmup set learning rate to 0.3342618384401114.\n",
      "Epoch: 0, Batch: 239, Loss: 0.000000\n",
      "Epoch 241: LinearWarmup set learning rate to 0.33565459610027853.\n",
      "Epoch: 0, Batch: 240, Loss: 0.577094\n",
      "Epoch 242: LinearWarmup set learning rate to 0.3370473537604457.\n",
      "Epoch: 0, Batch: 241, Loss: 0.000000\n",
      "Epoch 243: LinearWarmup set learning rate to 0.3384401114206128.\n",
      "Epoch: 0, Batch: 242, Loss: 0.000557\n",
      "Epoch 244: LinearWarmup set learning rate to 0.3398328690807799.\n",
      "Epoch: 0, Batch: 243, Loss: 0.000000\n",
      "Epoch 245: LinearWarmup set learning rate to 0.3412256267409471.\n",
      "Epoch: 0, Batch: 244, Loss: 0.004456\n",
      "Epoch 246: LinearWarmup set learning rate to 0.3426183844011142.\n",
      "Epoch: 0, Batch: 245, Loss: 0.001114\n",
      "Epoch 247: LinearWarmup set learning rate to 0.34401114206128136.\n",
      "Epoch: 0, Batch: 246, Loss: 0.000000\n",
      "Epoch 248: LinearWarmup set learning rate to 0.34540389972144847.\n",
      "Epoch: 0, Batch: 247, Loss: 0.009470\n",
      "Epoch 249: LinearWarmup set learning rate to 0.3467966573816156.\n",
      "Epoch: 0, Batch: 248, Loss: 0.000557\n",
      "Epoch 250: LinearWarmup set learning rate to 0.34818941504178275.\n",
      "Epoch: 0, Batch: 249, Loss: 0.118093\n",
      "Epoch 251: LinearWarmup set learning rate to 0.34958217270194986.\n",
      "Epoch: 0, Batch: 250, Loss: 0.000000\n",
      "Epoch 252: LinearWarmup set learning rate to 0.35097493036211697.\n",
      "Epoch: 0, Batch: 251, Loss: 0.000557\n",
      "Epoch 253: LinearWarmup set learning rate to 0.35236768802228413.\n",
      "Epoch: 0, Batch: 252, Loss: 0.000000\n",
      "Epoch 254: LinearWarmup set learning rate to 0.35376044568245124.\n",
      "Epoch: 0, Batch: 253, Loss: 0.000000\n",
      "Epoch 255: LinearWarmup set learning rate to 0.3551532033426184.\n",
      "Epoch: 0, Batch: 254, Loss: 0.000000\n",
      "Epoch 256: LinearWarmup set learning rate to 0.3565459610027855.\n",
      "Epoch: 0, Batch: 255, Loss: 0.000000\n",
      "Epoch 257: LinearWarmup set learning rate to 0.3579387186629526.\n",
      "Epoch: 0, Batch: 256, Loss: 0.017825\n",
      "Epoch 258: LinearWarmup set learning rate to 0.3593314763231198.\n",
      "Epoch: 0, Batch: 257, Loss: 0.001114\n",
      "Epoch 259: LinearWarmup set learning rate to 0.3607242339832869.\n",
      "Epoch: 0, Batch: 258, Loss: 0.005570\n",
      "Epoch 260: LinearWarmup set learning rate to 0.362116991643454.\n",
      "Epoch: 0, Batch: 259, Loss: 0.016154\n",
      "Epoch 261: LinearWarmup set learning rate to 0.3635097493036212.\n",
      "Epoch: 0, Batch: 260, Loss: 0.000000\n",
      "Epoch 262: LinearWarmup set learning rate to 0.3649025069637883.\n",
      "Epoch: 0, Batch: 261, Loss: 0.000000\n",
      "Epoch 263: LinearWarmup set learning rate to 0.36629526462395545.\n",
      "Epoch: 0, Batch: 262, Loss: 0.001671\n",
      "Epoch 264: LinearWarmup set learning rate to 0.36768802228412256.\n",
      "Epoch: 0, Batch: 263, Loss: 0.000000\n",
      "Epoch 265: LinearWarmup set learning rate to 0.36908077994428967.\n",
      "Epoch: 0, Batch: 264, Loss: 0.003899\n",
      "Epoch 266: LinearWarmup set learning rate to 0.37047353760445684.\n",
      "Epoch: 0, Batch: 265, Loss: 0.121435\n",
      "Epoch 267: LinearWarmup set learning rate to 0.37186629526462395.\n",
      "Epoch: 0, Batch: 266, Loss: 0.000000\n",
      "Epoch 268: LinearWarmup set learning rate to 0.3732590529247911.\n",
      "Epoch: 0, Batch: 267, Loss: 0.000000\n",
      "Epoch 269: LinearWarmup set learning rate to 0.3746518105849582.\n",
      "Epoch: 0, Batch: 268, Loss: 0.003342\n",
      "Epoch 270: LinearWarmup set learning rate to 0.37604456824512533.\n",
      "Epoch: 0, Batch: 269, Loss: 0.000000\n",
      "Epoch 271: LinearWarmup set learning rate to 0.3774373259052925.\n",
      "Epoch: 0, Batch: 270, Loss: 0.000000\n",
      "Epoch 272: LinearWarmup set learning rate to 0.3788300835654596.\n",
      "Epoch: 0, Batch: 271, Loss: 0.082999\n",
      "Epoch 273: LinearWarmup set learning rate to 0.3802228412256267.\n",
      "Epoch: 0, Batch: 272, Loss: 0.000000\n",
      "Epoch 274: LinearWarmup set learning rate to 0.3816155988857939.\n",
      "Epoch: 0, Batch: 273, Loss: 0.000000\n",
      "Epoch 275: LinearWarmup set learning rate to 0.383008356545961.\n",
      "Epoch: 0, Batch: 274, Loss: 0.000000\n",
      "Epoch 276: LinearWarmup set learning rate to 0.38440111420612816.\n",
      "Epoch: 0, Batch: 275, Loss: 0.000000\n",
      "Epoch 277: LinearWarmup set learning rate to 0.38579387186629527.\n",
      "Epoch: 0, Batch: 276, Loss: 0.000000\n",
      "Epoch 278: LinearWarmup set learning rate to 0.3871866295264624.\n",
      "Epoch: 0, Batch: 277, Loss: 0.000000\n",
      "Epoch 279: LinearWarmup set learning rate to 0.38857938718662954.\n",
      "Epoch: 0, Batch: 278, Loss: 0.000000\n",
      "Epoch 280: LinearWarmup set learning rate to 0.38997214484679665.\n",
      "Epoch: 0, Batch: 279, Loss: 0.000000\n",
      "Epoch 281: LinearWarmup set learning rate to 0.39136490250696376.\n",
      "Epoch: 0, Batch: 280, Loss: 0.045677\n",
      "Epoch 282: LinearWarmup set learning rate to 0.39275766016713093.\n",
      "Epoch: 0, Batch: 281, Loss: 0.000557\n",
      "Epoch 283: LinearWarmup set learning rate to 0.39415041782729804.\n",
      "Epoch: 0, Batch: 282, Loss: 0.000000\n",
      "Epoch 284: LinearWarmup set learning rate to 0.3955431754874652.\n",
      "Epoch: 0, Batch: 283, Loss: 0.000000\n",
      "Epoch 285: LinearWarmup set learning rate to 0.3969359331476323.\n",
      "Epoch: 0, Batch: 284, Loss: 0.000000\n",
      "Epoch 286: LinearWarmup set learning rate to 0.3983286908077994.\n",
      "Epoch: 0, Batch: 285, Loss: 0.000000\n",
      "Epoch 287: LinearWarmup set learning rate to 0.3997214484679666.\n",
      "Epoch: 0, Batch: 286, Loss: 0.000000\n",
      "Epoch 288: LinearWarmup set learning rate to 0.4011142061281337.\n",
      "Epoch: 0, Batch: 287, Loss: 0.000000\n",
      "Epoch 289: LinearWarmup set learning rate to 0.4025069637883008.\n",
      "Epoch: 0, Batch: 288, Loss: 0.000000\n",
      "Epoch 290: LinearWarmup set learning rate to 0.403899721448468.\n",
      "Epoch: 0, Batch: 289, Loss: 0.000557\n",
      "Epoch 291: LinearWarmup set learning rate to 0.4052924791086351.\n",
      "Epoch: 0, Batch: 290, Loss: 0.000000\n",
      "Epoch 292: LinearWarmup set learning rate to 0.40668523676880225.\n",
      "Epoch: 0, Batch: 291, Loss: 0.000000\n",
      "Epoch 293: LinearWarmup set learning rate to 0.40807799442896936.\n",
      "Epoch: 0, Batch: 292, Loss: 0.002785\n",
      "Epoch 294: LinearWarmup set learning rate to 0.40947075208913647.\n",
      "Epoch: 0, Batch: 293, Loss: 0.000000\n",
      "Epoch 295: LinearWarmup set learning rate to 0.41086350974930363.\n",
      "Epoch: 0, Batch: 294, Loss: 0.000000\n",
      "Epoch 296: LinearWarmup set learning rate to 0.41225626740947074.\n",
      "Epoch: 0, Batch: 295, Loss: 0.000000\n",
      "Epoch 297: LinearWarmup set learning rate to 0.4136490250696379.\n",
      "Epoch: 0, Batch: 296, Loss: 0.000000\n",
      "Epoch 298: LinearWarmup set learning rate to 0.415041782729805.\n",
      "Epoch: 0, Batch: 297, Loss: 0.000000\n",
      "Epoch 299: LinearWarmup set learning rate to 0.41643454038997213.\n",
      "Epoch: 0, Batch: 298, Loss: 0.000000\n",
      "Epoch 300: LinearWarmup set learning rate to 0.4178272980501393.\n",
      "Epoch: 0, Batch: 299, Loss: 0.000000\n",
      "Epoch 301: LinearWarmup set learning rate to 0.4192200557103064.\n",
      "Epoch: 0, Batch: 300, Loss: 0.000000\n",
      "Epoch 302: LinearWarmup set learning rate to 0.4206128133704735.\n",
      "Epoch: 0, Batch: 301, Loss: 0.000557\n",
      "Epoch 303: LinearWarmup set learning rate to 0.4220055710306407.\n",
      "Epoch: 0, Batch: 302, Loss: 0.000000\n",
      "Epoch 304: LinearWarmup set learning rate to 0.4233983286908078.\n",
      "Epoch: 0, Batch: 303, Loss: 0.000000\n",
      "Epoch 305: LinearWarmup set learning rate to 0.42479108635097496.\n",
      "Epoch: 0, Batch: 304, Loss: 0.000000\n",
      "Epoch 306: LinearWarmup set learning rate to 0.42618384401114207.\n",
      "Epoch: 0, Batch: 305, Loss: 0.000000\n",
      "Epoch 307: LinearWarmup set learning rate to 0.4275766016713092.\n",
      "Epoch: 0, Batch: 306, Loss: 0.000000\n",
      "Epoch 308: LinearWarmup set learning rate to 0.42896935933147634.\n",
      "Epoch: 0, Batch: 307, Loss: 0.000000\n",
      "Epoch 309: LinearWarmup set learning rate to 0.43036211699164345.\n",
      "Epoch: 0, Batch: 308, Loss: 0.089127\n",
      "Epoch 310: LinearWarmup set learning rate to 0.43175487465181056.\n",
      "Epoch: 0, Batch: 309, Loss: 0.000000\n",
      "Epoch 311: LinearWarmup set learning rate to 0.4331476323119777.\n",
      "Epoch: 0, Batch: 310, Loss: 0.000000\n",
      "Epoch 312: LinearWarmup set learning rate to 0.43454038997214484.\n",
      "Epoch: 0, Batch: 311, Loss: 0.000000\n",
      "Epoch 313: LinearWarmup set learning rate to 0.435933147632312.\n",
      "Epoch: 0, Batch: 312, Loss: 0.002228\n",
      "Epoch 314: LinearWarmup set learning rate to 0.4373259052924791.\n",
      "Epoch: 0, Batch: 313, Loss: 0.000000\n",
      "Epoch 315: LinearWarmup set learning rate to 0.4387186629526462.\n",
      "Epoch: 0, Batch: 314, Loss: 0.000557\n",
      "Epoch 316: LinearWarmup set learning rate to 0.4401114206128134.\n",
      "Epoch: 0, Batch: 315, Loss: 0.000000\n",
      "Epoch 317: LinearWarmup set learning rate to 0.4415041782729805.\n",
      "Epoch: 0, Batch: 316, Loss: 0.000000\n",
      "Epoch 318: LinearWarmup set learning rate to 0.4428969359331476.\n",
      "Epoch: 0, Batch: 317, Loss: 0.002228\n",
      "Epoch 319: LinearWarmup set learning rate to 0.44428969359331477.\n",
      "Epoch: 0, Batch: 318, Loss: 0.000000\n",
      "Epoch 320: LinearWarmup set learning rate to 0.4456824512534819.\n",
      "Epoch: 0, Batch: 319, Loss: 0.000557\n",
      "Epoch 321: LinearWarmup set learning rate to 0.44707520891364905.\n",
      "Epoch: 0, Batch: 320, Loss: 0.000000\n",
      "Epoch 322: LinearWarmup set learning rate to 0.44846796657381616.\n",
      "Epoch: 0, Batch: 321, Loss: 0.000000\n",
      "Epoch 323: LinearWarmup set learning rate to 0.44986072423398327.\n",
      "Epoch: 0, Batch: 322, Loss: 0.000000\n",
      "Epoch 324: LinearWarmup set learning rate to 0.45125348189415043.\n",
      "Epoch: 0, Batch: 323, Loss: 0.000000\n",
      "Epoch 325: LinearWarmup set learning rate to 0.45264623955431754.\n",
      "Epoch: 0, Batch: 324, Loss: 0.000000\n",
      "Epoch 326: LinearWarmup set learning rate to 0.45403899721448465.\n",
      "Epoch: 0, Batch: 325, Loss: 0.075758\n",
      "Epoch 327: LinearWarmup set learning rate to 0.4554317548746518.\n",
      "Epoch: 0, Batch: 326, Loss: 0.000000\n",
      "Epoch 328: LinearWarmup set learning rate to 0.4568245125348189.\n",
      "Epoch: 0, Batch: 327, Loss: 0.278520\n",
      "Epoch 329: LinearWarmup set learning rate to 0.4582172701949861.\n",
      "Epoch: 0, Batch: 328, Loss: 0.003899\n",
      "Epoch 330: LinearWarmup set learning rate to 0.4596100278551532.\n",
      "Epoch: 0, Batch: 329, Loss: 0.000000\n",
      "Epoch 331: LinearWarmup set learning rate to 0.4610027855153203.\n",
      "Epoch: 0, Batch: 330, Loss: 0.029523\n",
      "Epoch 332: LinearWarmup set learning rate to 0.4623955431754875.\n",
      "Epoch: 0, Batch: 331, Loss: 0.091355\n",
      "Epoch 333: LinearWarmup set learning rate to 0.4637883008356546.\n",
      "Epoch: 0, Batch: 332, Loss: 0.000000\n",
      "Epoch 334: LinearWarmup set learning rate to 0.46518105849582175.\n",
      "Epoch: 0, Batch: 333, Loss: 0.001114\n",
      "Epoch 335: LinearWarmup set learning rate to 0.46657381615598886.\n",
      "Epoch: 0, Batch: 334, Loss: 0.000000\n",
      "Epoch 336: LinearWarmup set learning rate to 0.467966573816156.\n",
      "Epoch: 0, Batch: 335, Loss: 0.000000\n",
      "Epoch 337: LinearWarmup set learning rate to 0.46935933147632314.\n",
      "Epoch: 0, Batch: 336, Loss: 0.120321\n",
      "Epoch 338: LinearWarmup set learning rate to 0.47075208913649025.\n",
      "Epoch: 0, Batch: 337, Loss: 0.010027\n",
      "Epoch 339: LinearWarmup set learning rate to 0.47214484679665736.\n",
      "Epoch: 0, Batch: 338, Loss: 0.056818\n",
      "Epoch 340: LinearWarmup set learning rate to 0.4735376044568245.\n",
      "Epoch: 0, Batch: 339, Loss: 0.003899\n",
      "Epoch 341: LinearWarmup set learning rate to 0.47493036211699163.\n",
      "Epoch: 0, Batch: 340, Loss: 0.000000\n",
      "Epoch 342: LinearWarmup set learning rate to 0.4763231197771588.\n",
      "Epoch: 0, Batch: 341, Loss: 0.118093\n",
      "Epoch 343: LinearWarmup set learning rate to 0.4777158774373259.\n",
      "Epoch: 0, Batch: 342, Loss: 0.000000\n",
      "Epoch 344: LinearWarmup set learning rate to 0.479108635097493.\n",
      "Epoch: 0, Batch: 343, Loss: 0.000000\n",
      "Epoch 345: LinearWarmup set learning rate to 0.4805013927576602.\n",
      "Epoch: 0, Batch: 344, Loss: 0.000000\n",
      "Epoch 346: LinearWarmup set learning rate to 0.4818941504178273.\n",
      "Epoch: 0, Batch: 345, Loss: 0.264037\n",
      "Epoch 347: LinearWarmup set learning rate to 0.4832869080779944.\n",
      "Epoch: 0, Batch: 346, Loss: 0.000000\n",
      "Epoch 348: LinearWarmup set learning rate to 0.48467966573816157.\n",
      "Epoch: 0, Batch: 347, Loss: 0.007242\n",
      "Epoch 349: LinearWarmup set learning rate to 0.4860724233983287.\n",
      "Epoch: 0, Batch: 348, Loss: 0.000000\n",
      "Epoch 350: LinearWarmup set learning rate to 0.48746518105849584.\n",
      "Epoch: 0, Batch: 349, Loss: 0.000557\n",
      "Epoch 351: LinearWarmup set learning rate to 0.48885793871866295.\n",
      "Epoch: 0, Batch: 350, Loss: 0.194407\n",
      "Epoch 352: LinearWarmup set learning rate to 0.49025069637883006.\n",
      "Epoch: 0, Batch: 351, Loss: 0.000557\n",
      "Epoch 353: LinearWarmup set learning rate to 0.49164345403899723.\n",
      "Epoch: 0, Batch: 352, Loss: 0.000000\n",
      "Epoch 354: LinearWarmup set learning rate to 0.49303621169916434.\n",
      "Epoch: 0, Batch: 353, Loss: 0.000000\n",
      "Epoch 355: LinearWarmup set learning rate to 0.49442896935933145.\n",
      "Epoch: 0, Batch: 354, Loss: 0.000000\n",
      "Epoch 356: LinearWarmup set learning rate to 0.4958217270194986.\n",
      "Epoch: 0, Batch: 355, Loss: 0.000000\n",
      "Epoch 357: LinearWarmup set learning rate to 0.4972144846796657.\n",
      "Epoch: 0, Batch: 356, Loss: 0.254011\n",
      "Epoch 358: LinearWarmup set learning rate to 0.4986072423398329.\n",
      "Epoch: 0, Batch: 357, Loss: 0.000000\n",
      "Epoch 359: LinearWarmup set learning rate to 1e-06.\n",
      "Epoch: 0, Batch: 358, Loss: 0.000000\n",
      "Epoch 0/1 completed.\n",
      "Average train_loss: 0.031033    Average train_accuracy: 0.999690\n",
      "Average val_loss: 0.014801    Average val_accuracy: 0.999852\n"
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "scheduler = paddle.optimizer.lr.LinearWarmup(learning_rate=1e-6, warmup_steps=359, start_lr=0, end_lr=0.5, verbose=True)\n",
    "train_model(mymodel_conv, times=1,\n",
    "            epochs=1,data_paths=data_paths,val_ratio=0.1,\n",
    "            lr_scheduler=scheduler,lr_shedular_type='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a099800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间戳类型: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "第一个NC文件可用变量: ['lai_lv', 'lai_hv', 'sp', 'v10', 'u10', 'skt', 'd2m', 'precipitation']\n",
      "第二个NC文件可用变量: ['v_850', 'v_500', 'u_850', 'u_500', 't_850', 't_500', 'q_850', 'q_500', 'z_850', 'z_500']\n",
      "=== 时间信息 ===\n",
      "时间范围: 2024-03-01 00:00:00 到 2025-02-28 00:00:00\n",
      "总时间步数: 365\n",
      "可用样本数: 359\n",
      "Epoch: 1, Batch: 0, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 1, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 3, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 4, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 5, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 6, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 7, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 8, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 10, Loss: 0.00946970\n",
      "Epoch: 1, Batch: 11, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 12, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 13, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 15, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 16, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 17, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 18, Loss: 0.26905081\n",
      "Epoch: 1, Batch: 19, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 20, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 21, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 23, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 24, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 25, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 26, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 29, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 30, Loss: 0.14037433\n",
      "Epoch: 1, Batch: 31, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 32, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 33, Loss: 0.25401071\n",
      "Epoch: 1, Batch: 35, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 36, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 37, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 38, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 39, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 40, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 42, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 43, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 45, Loss: 0.01615419\n",
      "Epoch: 1, Batch: 46, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 47, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 49, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 50, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 51, Loss: 0.00557041\n",
      "Epoch: 1, Batch: 52, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 53, Loss: 0.03899287\n",
      "Epoch: 1, Batch: 54, Loss: 0.00501337\n",
      "Epoch: 1, Batch: 55, Loss: 0.11809269\n",
      "Epoch: 1, Batch: 56, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 57, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 58, Loss: 0.05291890\n",
      "Epoch: 1, Batch: 59, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 60, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 61, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 62, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 63, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 64, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 65, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 66, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 67, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 68, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 69, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 70, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 71, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 72, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 73, Loss: 0.12143493\n",
      "Epoch: 1, Batch: 74, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 75, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 76, Loss: 0.12811942\n",
      "Epoch: 1, Batch: 77, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 78, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 79, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 80, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 81, Loss: 0.20833333\n",
      "Epoch: 1, Batch: 82, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 83, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 85, Loss: 0.88569516\n",
      "Epoch: 1, Batch: 86, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 87, Loss: 0.00724153\n",
      "Epoch: 1, Batch: 88, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 89, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 92, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 93, Loss: 0.57709450\n",
      "Epoch: 1, Batch: 94, Loss: 0.08912656\n",
      "Epoch: 1, Batch: 95, Loss: 0.00724153\n",
      "Epoch: 1, Batch: 96, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 97, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 98, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 99, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 100, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 101, Loss: 0.08299911\n",
      "Epoch: 1, Batch: 102, Loss: 0.27852049\n",
      "Epoch: 1, Batch: 103, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 105, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 106, Loss: 0.11140820\n",
      "Epoch: 1, Batch: 107, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 109, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 110, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 111, Loss: 0.04400624\n",
      "Epoch: 1, Batch: 112, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 113, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 114, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 115, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 116, Loss: 0.00946970\n",
      "Epoch: 1, Batch: 117, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 118, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 119, Loss: 0.41443852\n",
      "Epoch: 1, Batch: 120, Loss: 0.01058378\n",
      "Epoch: 1, Batch: 121, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 122, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 123, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 124, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 125, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 126, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 127, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 128, Loss: 0.09024064\n",
      "Epoch: 1, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 130, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 131, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 132, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 133, Loss: 0.01615419\n",
      "Epoch: 1, Batch: 135, Loss: 0.06740196\n",
      "Epoch: 1, Batch: 136, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 137, Loss: 0.00445633\n",
      "Epoch: 1, Batch: 138, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 139, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 140, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 141, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 142, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 143, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 145, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 146, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 147, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 148, Loss: 0.01169786\n",
      "Epoch: 1, Batch: 149, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 150, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 151, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 152, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 153, Loss: 0.12032086\n",
      "Epoch: 1, Batch: 154, Loss: 0.30135918\n",
      "Epoch: 1, Batch: 155, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 156, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 158, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 159, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 160, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 161, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 162, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 163, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 164, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 165, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 169, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 170, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 171, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 172, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 173, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 174, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 175, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 176, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 177, Loss: 0.05180481\n",
      "Epoch: 1, Batch: 178, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 179, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 180, Loss: 0.01782531\n",
      "Epoch: 1, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 183, Loss: 0.24621212\n",
      "Epoch: 1, Batch: 185, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 186, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 187, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 188, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 189, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 190, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 192, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 193, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 194, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 195, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 196, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 197, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 198, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 199, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 201, Loss: 0.09135472\n",
      "Epoch: 1, Batch: 202, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 203, Loss: 0.18995099\n",
      "Epoch: 1, Batch: 204, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 205, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 207, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 208, Loss: 0.00612745\n",
      "Epoch: 1, Batch: 209, Loss: 0.19440731\n",
      "Epoch: 1, Batch: 210, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 211, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 212, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 213, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 214, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 215, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 216, Loss: 0.00557041\n",
      "Epoch: 1, Batch: 217, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 218, Loss: 0.01615419\n",
      "Epoch: 1, Batch: 219, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 220, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 221, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 222, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 223, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 226, Loss: 0.69295901\n",
      "Epoch: 1, Batch: 227, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 228, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 229, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 230, Loss: 0.02450980\n",
      "Epoch: 1, Batch: 231, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 232, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 233, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 234, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 235, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 236, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 238, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 239, Loss: 0.04233512\n",
      "Epoch: 1, Batch: 240, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 241, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 242, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 243, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 244, Loss: 0.77707219\n",
      "Epoch: 1, Batch: 245, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 246, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 247, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 248, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 249, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 250, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 251, Loss: 0.08578432\n",
      "Epoch: 1, Batch: 252, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 253, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 254, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 255, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 256, Loss: 0.02450980\n",
      "Epoch: 1, Batch: 257, Loss: 0.96256685\n",
      "Epoch: 1, Batch: 258, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 259, Loss: 0.19050802\n",
      "Epoch: 1, Batch: 260, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 261, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 263, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 265, Loss: 0.15262923\n",
      "Epoch: 1, Batch: 266, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 267, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 268, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 271, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 272, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 273, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 274, Loss: 0.01114082\n",
      "Epoch: 1, Batch: 275, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 276, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 277, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 279, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 280, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 281, Loss: 0.17992425\n",
      "Epoch: 1, Batch: 282, Loss: 0.05681818\n",
      "Epoch: 1, Batch: 283, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 284, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 285, Loss: 0.11809269\n",
      "Epoch: 1, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 287, Loss: 0.00946970\n",
      "Epoch: 1, Batch: 288, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 291, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 293, Loss: 0.01002674\n",
      "Epoch: 1, Batch: 294, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 295, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 296, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 297, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 298, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 300, Loss: 0.10695187\n",
      "Epoch: 1, Batch: 302, Loss: 0.04567736\n",
      "Epoch: 1, Batch: 303, Loss: 0.44173351\n",
      "Epoch: 1, Batch: 305, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 306, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 307, Loss: 0.04901961\n",
      "Epoch: 1, Batch: 308, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 309, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 310, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 312, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 313, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 314, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 315, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 316, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 317, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 318, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 319, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 320, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 321, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 322, Loss: 0.13146168\n",
      "Epoch: 1, Batch: 323, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 324, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 325, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 326, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 327, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 328, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 329, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 330, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 331, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 332, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 333, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 334, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 335, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 336, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 337, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 338, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 339, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 340, Loss: 0.14427362\n",
      "Epoch: 1, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 343, Loss: 0.02952317\n",
      "Epoch: 1, Batch: 344, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 345, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 346, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 348, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 349, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 350, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 351, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 352, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 353, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 354, Loss: 0.07575758\n",
      "Epoch: 1, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 356, Loss: 0.08689839\n",
      "Epoch: 1, Batch: 357, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 358, Loss: 0.00000000\n",
      "Epoch 1/5 completed.\n",
      "Average train_loss: 0.02916213    Average train_accuracy: 0.99970877\n",
      "Average val_loss: 0.03211739    Average val_accuracy: 0.99967879\n",
      "Epoch: 2, Batch: 0, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 1, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 3, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 4, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 5, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 6, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 7, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 8, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 10, Loss: 0.01002674\n",
      "Epoch: 2, Batch: 11, Loss: 0.04567736\n",
      "Epoch: 2, Batch: 12, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 13, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 15, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 16, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 17, Loss: 0.16989751\n",
      "Epoch: 2, Batch: 18, Loss: 0.01615419\n",
      "Epoch: 2, Batch: 19, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 20, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 21, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 23, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 24, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 25, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 26, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 29, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 30, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 31, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 32, Loss: 0.19440731\n",
      "Epoch: 2, Batch: 33, Loss: 0.00334225\n",
      "Epoch: 2, Batch: 35, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 36, Loss: 0.00389929\n",
      "Epoch: 2, Batch: 37, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 38, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 39, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 40, Loss: 0.19050802\n",
      "Epoch: 2, Batch: 42, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 43, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 45, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 46, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 47, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 49, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 50, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 51, Loss: 0.09024064\n",
      "Epoch: 2, Batch: 52, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 53, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 54, Loss: 0.00222816\n",
      "Epoch: 2, Batch: 55, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 56, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 57, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 58, Loss: 0.00222816\n",
      "Epoch: 2, Batch: 59, Loss: 0.26403743\n",
      "Epoch: 2, Batch: 60, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 61, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 62, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 63, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 64, Loss: 0.09135472\n",
      "Epoch: 2, Batch: 65, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 66, Loss: 0.04233512\n",
      "Epoch: 2, Batch: 67, Loss: 0.77707219\n",
      "Epoch: 2, Batch: 68, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 69, Loss: 0.00445633\n",
      "Epoch: 2, Batch: 70, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 71, Loss: 0.04901961\n",
      "Epoch: 2, Batch: 72, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 73, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 74, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 75, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 76, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 77, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 78, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 79, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 80, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 81, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 82, Loss: 0.01448307\n",
      "Epoch: 2, Batch: 83, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 85, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 86, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 87, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 88, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 89, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 92, Loss: 0.01615419\n",
      "Epoch: 2, Batch: 93, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 94, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 95, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 96, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 97, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 98, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 99, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 100, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 101, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 102, Loss: 0.25401071\n",
      "Epoch: 2, Batch: 103, Loss: 0.00501337\n",
      "Epoch: 2, Batch: 105, Loss: 0.00612745\n",
      "Epoch: 2, Batch: 106, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 107, Loss: 0.57709450\n",
      "Epoch: 2, Batch: 109, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 110, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 111, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 112, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 113, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 114, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 115, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 116, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 117, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 118, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 119, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 120, Loss: 0.01169786\n",
      "Epoch: 2, Batch: 121, Loss: 0.02952317\n",
      "Epoch: 2, Batch: 122, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 123, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 124, Loss: 0.00389929\n",
      "Epoch: 2, Batch: 125, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 126, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 127, Loss: 0.05291890\n",
      "Epoch: 2, Batch: 128, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 130, Loss: 0.08077095\n",
      "Epoch: 2, Batch: 131, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 132, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 133, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 135, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 136, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 137, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 138, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 139, Loss: 0.01169786\n",
      "Epoch: 2, Batch: 140, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 141, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 142, Loss: 0.01615419\n",
      "Epoch: 2, Batch: 143, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 145, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 146, Loss: 0.00222816\n",
      "Epoch: 2, Batch: 147, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 148, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 149, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 150, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 151, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 152, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 153, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 154, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 155, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 156, Loss: 0.10862300\n",
      "Epoch: 2, Batch: 158, Loss: 0.00946970\n",
      "Epoch: 2, Batch: 159, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 160, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 161, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 162, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 163, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 164, Loss: 0.02450980\n",
      "Epoch: 2, Batch: 165, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 169, Loss: 0.00724153\n",
      "Epoch: 2, Batch: 170, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 171, Loss: 0.00501337\n",
      "Epoch: 2, Batch: 172, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 173, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 174, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 175, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 176, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 177, Loss: 0.00222816\n",
      "Epoch: 2, Batch: 178, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 179, Loss: 0.15262923\n",
      "Epoch: 2, Batch: 180, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 183, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 185, Loss: 0.14427362\n",
      "Epoch: 2, Batch: 186, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 187, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 188, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 189, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 190, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 192, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 193, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 194, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 195, Loss: 0.04456328\n",
      "Epoch: 2, Batch: 196, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 197, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 198, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 199, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 201, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 202, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 203, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 204, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 205, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 207, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 208, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 209, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 210, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 211, Loss: 0.00445633\n",
      "Epoch: 2, Batch: 212, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 213, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 214, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 215, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 216, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 217, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 218, Loss: 0.00946970\n",
      "Epoch: 2, Batch: 219, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 220, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 221, Loss: 0.44173351\n",
      "Epoch: 2, Batch: 222, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 223, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 226, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 227, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 228, Loss: 0.00389929\n",
      "Epoch: 2, Batch: 229, Loss: 0.41443852\n",
      "Epoch: 2, Batch: 230, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 231, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 232, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 233, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 234, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 235, Loss: 0.30135918\n",
      "Epoch: 2, Batch: 236, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 238, Loss: 0.00334225\n",
      "Epoch: 2, Batch: 239, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 240, Loss: 0.01114082\n",
      "Epoch: 2, Batch: 241, Loss: 0.08299911\n",
      "Epoch: 2, Batch: 242, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 243, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 244, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 245, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 246, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 247, Loss: 0.11140820\n",
      "Epoch: 2, Batch: 248, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 249, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 250, Loss: 0.00946970\n",
      "Epoch: 2, Batch: 251, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 252, Loss: 0.00278520\n",
      "Epoch: 2, Batch: 253, Loss: 0.17992425\n",
      "Epoch: 2, Batch: 254, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 255, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 256, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 257, Loss: 0.00501337\n",
      "Epoch: 2, Batch: 258, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 259, Loss: 0.11809269\n",
      "Epoch: 2, Batch: 260, Loss: 0.01058378\n",
      "Epoch: 2, Batch: 261, Loss: 0.69295901\n",
      "Epoch: 2, Batch: 263, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 265, Loss: 0.00222816\n",
      "Epoch: 2, Batch: 266, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 267, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 268, Loss: 0.08689839\n",
      "Epoch: 2, Batch: 271, Loss: 0.06740196\n",
      "Epoch: 2, Batch: 272, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 273, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 274, Loss: 0.27852049\n",
      "Epoch: 2, Batch: 275, Loss: 0.12032086\n",
      "Epoch: 2, Batch: 276, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 277, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 279, Loss: 0.00278520\n",
      "Epoch: 2, Batch: 280, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 281, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 282, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 283, Loss: 0.24621212\n",
      "Epoch: 2, Batch: 284, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 285, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 287, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 288, Loss: 0.14037433\n",
      "Epoch: 2, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 291, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 293, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 294, Loss: 0.00557041\n",
      "Epoch: 2, Batch: 295, Loss: 0.05681818\n",
      "Epoch: 2, Batch: 296, Loss: 0.88569516\n",
      "Epoch: 2, Batch: 297, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 298, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 300, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 302, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 303, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 305, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 306, Loss: 0.01782531\n",
      "Epoch: 2, Batch: 307, Loss: 0.20833333\n",
      "Epoch: 2, Batch: 308, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 309, Loss: 0.08912656\n",
      "Epoch: 2, Batch: 310, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 312, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 313, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 314, Loss: 0.12811942\n",
      "Epoch: 2, Batch: 315, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 316, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 317, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 318, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 319, Loss: 0.08578432\n",
      "Epoch: 2, Batch: 320, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 321, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 322, Loss: 0.00724153\n",
      "Epoch: 2, Batch: 323, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 324, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 325, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 326, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 327, Loss: 0.12143493\n",
      "Epoch: 2, Batch: 328, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 329, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 330, Loss: 0.05180481\n",
      "Epoch: 2, Batch: 331, Loss: 0.18995099\n",
      "Epoch: 2, Batch: 332, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 333, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 334, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 335, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 336, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 337, Loss: 0.10695187\n",
      "Epoch: 2, Batch: 338, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 339, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 340, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 343, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 344, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 345, Loss: 0.00167112\n",
      "Epoch: 2, Batch: 346, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 348, Loss: 0.00334225\n",
      "Epoch: 2, Batch: 349, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 350, Loss: 0.00111408\n",
      "Epoch: 2, Batch: 351, Loss: 0.11809269\n",
      "Epoch: 2, Batch: 352, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 353, Loss: 0.07575758\n",
      "Epoch: 2, Batch: 354, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 356, Loss: 0.00055704\n",
      "Epoch: 2, Batch: 357, Loss: 0.00000000\n",
      "Epoch: 2, Batch: 358, Loss: 0.00000000\n",
      "Epoch 2/5 completed.\n",
      "Average train_loss: 0.03138170    Average train_accuracy: 0.99968666\n",
      "Average val_loss: 0.01157054    Average val_accuracy: 0.99988437\n",
      "Epoch: 3, Batch: 0, Loss: 0.03899287\n",
      "Epoch: 3, Batch: 1, Loss: 0.11809269\n",
      "Epoch: 3, Batch: 3, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 4, Loss: 0.04233512\n",
      "Epoch: 3, Batch: 5, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 6, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 7, Loss: 0.26403743\n",
      "Epoch: 3, Batch: 8, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 10, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 11, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 12, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 13, Loss: 0.00278520\n",
      "Epoch: 3, Batch: 15, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 16, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 17, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 18, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 19, Loss: 0.02450980\n",
      "Epoch: 3, Batch: 20, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 21, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 23, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 24, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 25, Loss: 0.13146168\n",
      "Epoch: 3, Batch: 26, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 29, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 30, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 31, Loss: 0.08299911\n",
      "Epoch: 3, Batch: 32, Loss: 0.04567736\n",
      "Epoch: 3, Batch: 33, Loss: 0.17992425\n",
      "Epoch: 3, Batch: 35, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 36, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 37, Loss: 0.88569516\n",
      "Epoch: 3, Batch: 38, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 39, Loss: 0.01782531\n",
      "Epoch: 3, Batch: 40, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 42, Loss: 0.04400624\n",
      "Epoch: 3, Batch: 43, Loss: 0.12143493\n",
      "Epoch: 3, Batch: 45, Loss: 0.00946970\n",
      "Epoch: 3, Batch: 46, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 47, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 49, Loss: 0.24621212\n",
      "Epoch: 3, Batch: 50, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 51, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 52, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 53, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 54, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 55, Loss: 0.00557041\n",
      "Epoch: 3, Batch: 56, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 57, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 58, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 59, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 60, Loss: 0.77707219\n",
      "Epoch: 3, Batch: 61, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 62, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 63, Loss: 0.57709450\n",
      "Epoch: 3, Batch: 64, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 65, Loss: 0.00724153\n",
      "Epoch: 3, Batch: 66, Loss: 0.11809269\n",
      "Epoch: 3, Batch: 67, Loss: 0.11140820\n",
      "Epoch: 3, Batch: 68, Loss: 0.00389929\n",
      "Epoch: 3, Batch: 69, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 70, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 71, Loss: 0.08689839\n",
      "Epoch: 3, Batch: 72, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 73, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 74, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 75, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 76, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 77, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 78, Loss: 0.10695187\n",
      "Epoch: 3, Batch: 79, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 80, Loss: 0.00278520\n",
      "Epoch: 3, Batch: 81, Loss: 0.07575758\n",
      "Epoch: 3, Batch: 82, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 83, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 85, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 86, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 87, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 88, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 89, Loss: 0.10862300\n",
      "Epoch: 3, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 92, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 93, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 94, Loss: 0.00222816\n",
      "Epoch: 3, Batch: 95, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 96, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 97, Loss: 0.02952317\n",
      "Epoch: 3, Batch: 98, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 99, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 100, Loss: 0.00557041\n",
      "Epoch: 3, Batch: 101, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 102, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 103, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 105, Loss: 0.08578432\n",
      "Epoch: 3, Batch: 106, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 107, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 109, Loss: 0.00445633\n",
      "Epoch: 3, Batch: 110, Loss: 0.00389929\n",
      "Epoch: 3, Batch: 111, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 112, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 113, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 114, Loss: 0.01615419\n",
      "Epoch: 3, Batch: 115, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 116, Loss: 0.19050802\n",
      "Epoch: 3, Batch: 117, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 118, Loss: 0.01615419\n",
      "Epoch: 3, Batch: 119, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 120, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 121, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 122, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 123, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 124, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 125, Loss: 0.04456328\n",
      "Epoch: 3, Batch: 126, Loss: 0.44173351\n",
      "Epoch: 3, Batch: 127, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 128, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 130, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 131, Loss: 0.00389929\n",
      "Epoch: 3, Batch: 132, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 133, Loss: 0.19440731\n",
      "Epoch: 3, Batch: 135, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 136, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 137, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 138, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 139, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 140, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 141, Loss: 0.05681818\n",
      "Epoch: 3, Batch: 142, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 143, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 145, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 146, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 147, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 148, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 149, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 150, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 151, Loss: 0.27852049\n",
      "Epoch: 3, Batch: 152, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 153, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 154, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 155, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 156, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 158, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 159, Loss: 0.00222816\n",
      "Epoch: 3, Batch: 160, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 161, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 162, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 163, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 164, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 165, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 169, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 170, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 171, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 172, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 173, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 174, Loss: 0.01169786\n",
      "Epoch: 3, Batch: 175, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 176, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 177, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 178, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 179, Loss: 0.01058378\n",
      "Epoch: 3, Batch: 180, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 183, Loss: 0.15262923\n",
      "Epoch: 3, Batch: 185, Loss: 0.00278520\n",
      "Epoch: 3, Batch: 186, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 187, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 188, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 189, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 190, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 192, Loss: 0.09024064\n",
      "Epoch: 3, Batch: 193, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 194, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 195, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 196, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 197, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 198, Loss: 0.00724153\n",
      "Epoch: 3, Batch: 199, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 201, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 202, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 203, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 204, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 205, Loss: 0.41443852\n",
      "Epoch: 3, Batch: 207, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 208, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 209, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 210, Loss: 0.12032086\n",
      "Epoch: 3, Batch: 211, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 212, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 213, Loss: 0.08077095\n",
      "Epoch: 3, Batch: 214, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 215, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 216, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 217, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 218, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 219, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 220, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 221, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 222, Loss: 0.00334225\n",
      "Epoch: 3, Batch: 223, Loss: 0.16989751\n",
      "Epoch: 3, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 226, Loss: 0.04901961\n",
      "Epoch: 3, Batch: 227, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 228, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 229, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 230, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 231, Loss: 0.00445633\n",
      "Epoch: 3, Batch: 232, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 233, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 234, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 235, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 236, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 238, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 239, Loss: 0.01169786\n",
      "Epoch: 3, Batch: 240, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 241, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 242, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 243, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 244, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 245, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 246, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 247, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 248, Loss: 0.00389929\n",
      "Epoch: 3, Batch: 249, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 250, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 251, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 252, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 253, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 254, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 255, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 256, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 257, Loss: 0.00334225\n",
      "Epoch: 3, Batch: 258, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 259, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 260, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 261, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 263, Loss: 0.01114082\n",
      "Epoch: 3, Batch: 265, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 266, Loss: 0.96256685\n",
      "Epoch: 3, Batch: 267, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 268, Loss: 0.69295901\n",
      "Epoch: 3, Batch: 271, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 272, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 273, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 274, Loss: 0.01615419\n",
      "Epoch: 3, Batch: 275, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 276, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 277, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 279, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 280, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 281, Loss: 0.00334225\n",
      "Epoch: 3, Batch: 282, Loss: 0.00612745\n",
      "Epoch: 3, Batch: 283, Loss: 0.18995099\n",
      "Epoch: 3, Batch: 284, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 285, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 287, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 288, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 291, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 293, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 294, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 295, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 296, Loss: 0.20833333\n",
      "Epoch: 3, Batch: 297, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 298, Loss: 0.02450980\n",
      "Epoch: 3, Batch: 300, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 302, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 303, Loss: 0.05291890\n",
      "Epoch: 3, Batch: 305, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 306, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 307, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 308, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 309, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 310, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 312, Loss: 0.00501337\n",
      "Epoch: 3, Batch: 313, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 314, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 315, Loss: 0.00946970\n",
      "Epoch: 3, Batch: 316, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 317, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 318, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 319, Loss: 0.00501337\n",
      "Epoch: 3, Batch: 320, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 321, Loss: 0.00111408\n",
      "Epoch: 3, Batch: 322, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 323, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 324, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 325, Loss: 0.25401071\n",
      "Epoch: 3, Batch: 326, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 327, Loss: 0.00167112\n",
      "Epoch: 3, Batch: 328, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 329, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 330, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 331, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 332, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 333, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 334, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 335, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 336, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 337, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 338, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 339, Loss: 0.00055704\n",
      "Epoch: 3, Batch: 340, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 343, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 344, Loss: 0.01002674\n",
      "Epoch: 3, Batch: 345, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 346, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 348, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 349, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 350, Loss: 0.05180481\n",
      "Epoch: 3, Batch: 351, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 352, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 353, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 354, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 356, Loss: 0.00000000\n",
      "Epoch: 3, Batch: 357, Loss: 0.01448307\n",
      "Epoch: 3, Batch: 358, Loss: 0.00111408\n",
      "Epoch 3/5 completed.\n",
      "Average train_loss: 0.02837643    Average train_accuracy: 0.99971652\n",
      "Average val_loss: 0.03939076    Average val_accuracy: 0.99960601\n",
      "Epoch: 4, Batch: 0, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 1, Loss: 0.08912656\n",
      "Epoch: 4, Batch: 3, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 4, Loss: 0.24621212\n",
      "Epoch: 4, Batch: 5, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 6, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 7, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 8, Loss: 0.00612745\n",
      "Epoch: 4, Batch: 10, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 11, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 12, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 13, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 15, Loss: 0.00501337\n",
      "Epoch: 4, Batch: 16, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 17, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 18, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 19, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 20, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 21, Loss: 0.09135472\n",
      "Epoch: 4, Batch: 23, Loss: 0.17992425\n",
      "Epoch: 4, Batch: 24, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 25, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 26, Loss: 0.57709450\n",
      "Epoch: 4, Batch: 29, Loss: 0.04567736\n",
      "Epoch: 4, Batch: 30, Loss: 0.10695187\n",
      "Epoch: 4, Batch: 31, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 32, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 33, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 35, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 36, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 37, Loss: 0.04456328\n",
      "Epoch: 4, Batch: 38, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 39, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 40, Loss: 0.01615419\n",
      "Epoch: 4, Batch: 42, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 43, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 45, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 46, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 47, Loss: 0.77707219\n",
      "Epoch: 4, Batch: 49, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 50, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 51, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 52, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 53, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 54, Loss: 0.15262923\n",
      "Epoch: 4, Batch: 55, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 56, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 57, Loss: 0.01448307\n",
      "Epoch: 4, Batch: 58, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 59, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 60, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 61, Loss: 0.00389929\n",
      "Epoch: 4, Batch: 62, Loss: 0.01114082\n",
      "Epoch: 4, Batch: 63, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 64, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 65, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 66, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 67, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 68, Loss: 0.11140820\n",
      "Epoch: 4, Batch: 69, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 70, Loss: 0.19440731\n",
      "Epoch: 4, Batch: 71, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 72, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 73, Loss: 0.01169786\n",
      "Epoch: 4, Batch: 74, Loss: 0.00334225\n",
      "Epoch: 4, Batch: 75, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 76, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 77, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 78, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 79, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 80, Loss: 0.00167112\n",
      "Epoch: 4, Batch: 81, Loss: 0.26905081\n",
      "Epoch: 4, Batch: 82, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 83, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 85, Loss: 0.00222816\n",
      "Epoch: 4, Batch: 86, Loss: 0.00334225\n",
      "Epoch: 4, Batch: 87, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 88, Loss: 0.00389929\n",
      "Epoch: 4, Batch: 89, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 92, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 93, Loss: 0.18995099\n",
      "Epoch: 4, Batch: 94, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 95, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 96, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 97, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 98, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 99, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 100, Loss: 0.00946970\n",
      "Epoch: 4, Batch: 101, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 102, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 103, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 105, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 106, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 107, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 109, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 110, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 111, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 112, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 113, Loss: 0.00278520\n",
      "Epoch: 4, Batch: 114, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 115, Loss: 0.09024064\n",
      "Epoch: 4, Batch: 116, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 117, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 118, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 119, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 120, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 121, Loss: 0.04901961\n",
      "Epoch: 4, Batch: 122, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 123, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 124, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 125, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 126, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 127, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 128, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 130, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 131, Loss: 0.44173351\n",
      "Epoch: 4, Batch: 132, Loss: 0.00167112\n",
      "Epoch: 4, Batch: 133, Loss: 0.00557041\n",
      "Epoch: 4, Batch: 135, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 136, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 137, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 138, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 139, Loss: 0.12032086\n",
      "Epoch: 4, Batch: 140, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 141, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 142, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 143, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 145, Loss: 0.00445633\n",
      "Epoch: 4, Batch: 146, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 147, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 148, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 149, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 150, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 151, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 152, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 153, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 154, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 155, Loss: 0.30135918\n",
      "Epoch: 4, Batch: 156, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 158, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 159, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 160, Loss: 0.04233512\n",
      "Epoch: 4, Batch: 161, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 162, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 163, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 164, Loss: 0.01782531\n",
      "Epoch: 4, Batch: 165, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 169, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 170, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 171, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 172, Loss: 0.00501337\n",
      "Epoch: 4, Batch: 173, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 174, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 175, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 176, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 177, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 178, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 179, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 180, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 183, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 185, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 186, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 187, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 188, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 189, Loss: 0.10862300\n",
      "Epoch: 4, Batch: 190, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 192, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 193, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 194, Loss: 0.00278520\n",
      "Epoch: 4, Batch: 195, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 196, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 197, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 198, Loss: 0.07575758\n",
      "Epoch: 4, Batch: 199, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 201, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 202, Loss: 0.08578432\n",
      "Epoch: 4, Batch: 203, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 204, Loss: 0.27852049\n",
      "Epoch: 4, Batch: 205, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 207, Loss: 0.41443852\n",
      "Epoch: 4, Batch: 208, Loss: 0.01058378\n",
      "Epoch: 4, Batch: 209, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 210, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 211, Loss: 0.00222816\n",
      "Epoch: 4, Batch: 212, Loss: 0.01615419\n",
      "Epoch: 4, Batch: 213, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 214, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 215, Loss: 0.14037433\n",
      "Epoch: 4, Batch: 216, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 217, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 218, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 219, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 220, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 221, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 222, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 223, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 226, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 227, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 228, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 229, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 230, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 231, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 232, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 233, Loss: 0.20833333\n",
      "Epoch: 4, Batch: 234, Loss: 0.00167112\n",
      "Epoch: 4, Batch: 235, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 236, Loss: 0.00222816\n",
      "Epoch: 4, Batch: 238, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 239, Loss: 0.00389929\n",
      "Epoch: 4, Batch: 240, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 241, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 242, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 243, Loss: 0.00167112\n",
      "Epoch: 4, Batch: 244, Loss: 0.96256685\n",
      "Epoch: 4, Batch: 245, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 246, Loss: 0.00167112\n",
      "Epoch: 4, Batch: 247, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 248, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 249, Loss: 0.19050802\n",
      "Epoch: 4, Batch: 250, Loss: 0.01002674\n",
      "Epoch: 4, Batch: 251, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 252, Loss: 0.16989751\n",
      "Epoch: 4, Batch: 253, Loss: 0.00389929\n",
      "Epoch: 4, Batch: 254, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 255, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 256, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 257, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 258, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 259, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 260, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 261, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 263, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 265, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 266, Loss: 0.69295901\n",
      "Epoch: 4, Batch: 267, Loss: 0.02450980\n",
      "Epoch: 4, Batch: 268, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 271, Loss: 0.04400624\n",
      "Epoch: 4, Batch: 272, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 273, Loss: 0.12811942\n",
      "Epoch: 4, Batch: 274, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 275, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 276, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 277, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 279, Loss: 0.06740196\n",
      "Epoch: 4, Batch: 280, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 281, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 282, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 283, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 284, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 285, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 287, Loss: 0.08689839\n",
      "Epoch: 4, Batch: 288, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 291, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 293, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 294, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 295, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 296, Loss: 0.02952317\n",
      "Epoch: 4, Batch: 297, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 298, Loss: 0.11809269\n",
      "Epoch: 4, Batch: 300, Loss: 0.00946970\n",
      "Epoch: 4, Batch: 302, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 303, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 305, Loss: 0.01615419\n",
      "Epoch: 4, Batch: 306, Loss: 0.00445633\n",
      "Epoch: 4, Batch: 307, Loss: 0.03899287\n",
      "Epoch: 4, Batch: 308, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 309, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 310, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 312, Loss: 0.01169786\n",
      "Epoch: 4, Batch: 313, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 314, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 315, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 316, Loss: 0.02450980\n",
      "Epoch: 4, Batch: 317, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 318, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 319, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 320, Loss: 0.13146168\n",
      "Epoch: 4, Batch: 321, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 322, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 323, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 324, Loss: 0.12143493\n",
      "Epoch: 4, Batch: 325, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 326, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 327, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 328, Loss: 0.00724153\n",
      "Epoch: 4, Batch: 329, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 330, Loss: 0.05291890\n",
      "Epoch: 4, Batch: 331, Loss: 0.00557041\n",
      "Epoch: 4, Batch: 332, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 333, Loss: 0.88569516\n",
      "Epoch: 4, Batch: 334, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 335, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 336, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 337, Loss: 0.05180481\n",
      "Epoch: 4, Batch: 338, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 339, Loss: 0.08077095\n",
      "Epoch: 4, Batch: 340, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 343, Loss: 0.14427362\n",
      "Epoch: 4, Batch: 344, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 345, Loss: 0.00222816\n",
      "Epoch: 4, Batch: 346, Loss: 0.11809269\n",
      "Epoch: 4, Batch: 348, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 349, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 350, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 351, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 352, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 353, Loss: 0.00111408\n",
      "Epoch: 4, Batch: 354, Loss: 0.00222816\n",
      "Epoch: 4, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 4, Batch: 356, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 357, Loss: 0.00055704\n",
      "Epoch: 4, Batch: 358, Loss: 0.00000000\n",
      "Epoch 4/5 completed.\n",
      "Average train_loss: 0.03079887    Average train_accuracy: 0.99969238\n",
      "Average val_loss: 0.01696588    Average val_accuracy: 0.99983042\n",
      "Epoch: 5, Batch: 0, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 1, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 3, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 4, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 5, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 6, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 7, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 8, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 10, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 11, Loss: 0.02952317\n",
      "Epoch: 5, Batch: 12, Loss: 0.14427362\n",
      "Epoch: 5, Batch: 13, Loss: 0.00222816\n",
      "Epoch: 5, Batch: 15, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 16, Loss: 0.44173351\n",
      "Epoch: 5, Batch: 17, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 18, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 19, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 20, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 21, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 23, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 24, Loss: 0.04400624\n",
      "Epoch: 5, Batch: 25, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 26, Loss: 0.07575758\n",
      "Epoch: 5, Batch: 29, Loss: 0.00501337\n",
      "Epoch: 5, Batch: 30, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 31, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 32, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 33, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 35, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 36, Loss: 0.00278520\n",
      "Epoch: 5, Batch: 37, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 38, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 39, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 40, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 42, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 43, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 45, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 46, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 47, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 49, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 50, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 51, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 52, Loss: 0.01615419\n",
      "Epoch: 5, Batch: 53, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 54, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 55, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 56, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 57, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 58, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 59, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 60, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 61, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 62, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 63, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 64, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 65, Loss: 0.09024064\n",
      "Epoch: 5, Batch: 66, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 67, Loss: 0.19440731\n",
      "Epoch: 5, Batch: 68, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 69, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 70, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 71, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 72, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 73, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 74, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 75, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 76, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 77, Loss: 0.00222816\n",
      "Epoch: 5, Batch: 78, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 79, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 80, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 81, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 82, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 83, Loss: 0.00278520\n",
      "Epoch: 5, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 85, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 86, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 87, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 88, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 89, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 92, Loss: 0.00557041\n",
      "Epoch: 5, Batch: 93, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 94, Loss: 0.00334225\n",
      "Epoch: 5, Batch: 95, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 96, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 97, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 98, Loss: 0.00501337\n",
      "Epoch: 5, Batch: 99, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 100, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 101, Loss: 0.00222816\n",
      "Epoch: 5, Batch: 102, Loss: 0.00724153\n",
      "Epoch: 5, Batch: 103, Loss: 0.20833333\n",
      "Epoch: 5, Batch: 105, Loss: 0.00724153\n",
      "Epoch: 5, Batch: 106, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 107, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 109, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 110, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 111, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 112, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 113, Loss: 0.08689839\n",
      "Epoch: 5, Batch: 114, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 115, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 116, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 117, Loss: 0.00946970\n",
      "Epoch: 5, Batch: 118, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 119, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 120, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 121, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 122, Loss: 0.27852049\n",
      "Epoch: 5, Batch: 123, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 124, Loss: 0.69295901\n",
      "Epoch: 5, Batch: 125, Loss: 0.02450980\n",
      "Epoch: 5, Batch: 126, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 127, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 128, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 130, Loss: 0.06740196\n",
      "Epoch: 5, Batch: 131, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 132, Loss: 0.00222816\n",
      "Epoch: 5, Batch: 133, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 135, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 136, Loss: 0.12811942\n",
      "Epoch: 5, Batch: 137, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 138, Loss: 0.01002674\n",
      "Epoch: 5, Batch: 139, Loss: 0.12032086\n",
      "Epoch: 5, Batch: 140, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 141, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 142, Loss: 0.08077095\n",
      "Epoch: 5, Batch: 143, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 145, Loss: 0.15262923\n",
      "Epoch: 5, Batch: 146, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 147, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 148, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 149, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 150, Loss: 0.11809269\n",
      "Epoch: 5, Batch: 151, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 152, Loss: 0.08912656\n",
      "Epoch: 5, Batch: 153, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 154, Loss: 0.11140820\n",
      "Epoch: 5, Batch: 155, Loss: 0.00278520\n",
      "Epoch: 5, Batch: 156, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 158, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 159, Loss: 0.01615419\n",
      "Epoch: 5, Batch: 160, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 161, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 162, Loss: 0.01782531\n",
      "Epoch: 5, Batch: 163, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 164, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 165, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 169, Loss: 0.05291890\n",
      "Epoch: 5, Batch: 170, Loss: 0.00334225\n",
      "Epoch: 5, Batch: 171, Loss: 0.01058378\n",
      "Epoch: 5, Batch: 172, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 173, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 174, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 175, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 176, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 177, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 178, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 179, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 180, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 183, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 185, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 186, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 187, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 188, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 189, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 190, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 192, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 193, Loss: 0.19050802\n",
      "Epoch: 5, Batch: 194, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 195, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 196, Loss: 0.25401071\n",
      "Epoch: 5, Batch: 197, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 198, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 199, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 201, Loss: 0.00389929\n",
      "Epoch: 5, Batch: 202, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 203, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 204, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 205, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 207, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 208, Loss: 0.00445633\n",
      "Epoch: 5, Batch: 209, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 210, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 211, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 212, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 213, Loss: 0.01615419\n",
      "Epoch: 5, Batch: 214, Loss: 0.10862300\n",
      "Epoch: 5, Batch: 215, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 216, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 217, Loss: 0.01448307\n",
      "Epoch: 5, Batch: 218, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 219, Loss: 0.17992425\n",
      "Epoch: 5, Batch: 220, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 221, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 222, Loss: 0.01114082\n",
      "Epoch: 5, Batch: 223, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 226, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 227, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 228, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 229, Loss: 0.88569516\n",
      "Epoch: 5, Batch: 230, Loss: 0.05180481\n",
      "Epoch: 5, Batch: 231, Loss: 0.04567736\n",
      "Epoch: 5, Batch: 232, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 233, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 234, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 235, Loss: 0.57709450\n",
      "Epoch: 5, Batch: 236, Loss: 0.11809269\n",
      "Epoch: 5, Batch: 238, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 239, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 240, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 241, Loss: 0.00612745\n",
      "Epoch: 5, Batch: 242, Loss: 0.04456328\n",
      "Epoch: 5, Batch: 243, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 244, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 245, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 246, Loss: 0.00222816\n",
      "Epoch: 5, Batch: 247, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 248, Loss: 0.01169786\n",
      "Epoch: 5, Batch: 249, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 250, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 251, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 252, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 253, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 254, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 255, Loss: 0.04233512\n",
      "Epoch: 5, Batch: 256, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 257, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 258, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 259, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 260, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 261, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 263, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 265, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 266, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 267, Loss: 0.26403743\n",
      "Epoch: 5, Batch: 268, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 271, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 272, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 273, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 274, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 275, Loss: 0.41443852\n",
      "Epoch: 5, Batch: 276, Loss: 0.13146168\n",
      "Epoch: 5, Batch: 277, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 279, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 280, Loss: 0.00389929\n",
      "Epoch: 5, Batch: 281, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 282, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 283, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 284, Loss: 0.26905081\n",
      "Epoch: 5, Batch: 285, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 287, Loss: 0.08299911\n",
      "Epoch: 5, Batch: 288, Loss: 0.00946970\n",
      "Epoch: 5, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 291, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 293, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 294, Loss: 0.00334225\n",
      "Epoch: 5, Batch: 295, Loss: 0.96256685\n",
      "Epoch: 5, Batch: 296, Loss: 0.00557041\n",
      "Epoch: 5, Batch: 297, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 298, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 300, Loss: 0.08578432\n",
      "Epoch: 5, Batch: 302, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 303, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 305, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 306, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 307, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 308, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 309, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 310, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 312, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 313, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 314, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 315, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 316, Loss: 0.00389929\n",
      "Epoch: 5, Batch: 317, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 318, Loss: 0.02450980\n",
      "Epoch: 5, Batch: 319, Loss: 0.03899287\n",
      "Epoch: 5, Batch: 320, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 321, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 322, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 323, Loss: 0.00111408\n",
      "Epoch: 5, Batch: 324, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 325, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 326, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 327, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 328, Loss: 0.05681818\n",
      "Epoch: 5, Batch: 329, Loss: 0.30135918\n",
      "Epoch: 5, Batch: 330, Loss: 0.00055704\n",
      "Epoch: 5, Batch: 331, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 332, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 333, Loss: 0.04901961\n",
      "Epoch: 5, Batch: 334, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 335, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 336, Loss: 0.00167112\n",
      "Epoch: 5, Batch: 337, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 338, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 339, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 340, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 343, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 344, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 345, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 346, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 348, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 349, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 350, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 351, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 352, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 353, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 354, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 356, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 357, Loss: 0.00000000\n",
      "Epoch: 5, Batch: 358, Loss: 0.00000000\n",
      "Epoch 5/5 completed.\n",
      "Average train_loss: 0.02955756    Average train_accuracy: 0.99970472\n",
      "Average val_loss: 0.02845684    Average val_accuracy: 0.99971557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FireRiskWithoutNWP(\n",
       "  (adapter): ChannelMlp(\n",
       "    (linear1): Conv2D(58, 64, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (linear2): Conv2D(64, 25, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (mlp): ChannelMlp(\n",
       "    (linear1): Conv2D(48, 128, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (linear2): Conv2D(128, 58, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (segmenthead): BinarySegmentationHead(\n",
       "    (conv): Conv2D(58, 1, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (yinglong): Sequential(\n",
       "    (0): Conv2D(25, 36, kernel_size=[5, 5], padding=same, data_format=NCHW)\n",
       "    (1): Conv2D(36, 48, kernel_size=[5, 5], padding=same, data_format=NCHW)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(mymodel_conv, times=2,\n",
    "            epochs=5,data_paths=data_paths,val_ratio=0.1,\n",
    "            lr_scheduler=0.23,lr_shedular_type='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c4960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间戳类型: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "第一个NC文件可用变量: ['lai_lv', 'lai_hv', 'sp', 'v10', 'u10', 'skt', 'd2m', 'precipitation']\n",
      "第二个NC文件可用变量: ['v_850', 'v_500', 'u_850', 'u_500', 't_850', 't_500', 'q_850', 'q_500', 'z_850', 'z_500']\n",
      "=== 时间信息 ===\n",
      "时间范围: 2024-03-01 00:00:00 到 2025-02-28 00:00:00\n",
      "总时间步数: 365\n",
      "可用样本数: 359\n",
      "Epoch: 1, Batch: 0, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 1, Loss: 0.12811942\n",
      "Epoch: 1, Batch: 2, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 3, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 4, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 5, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 6, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 7, Loss: 0.01002674\n",
      "Epoch: 1, Batch: 9, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 10, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 11, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 12, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 13, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 14, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 15, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 16, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 17, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 18, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 19, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 20, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 21, Loss: 0.11809269\n",
      "Epoch: 1, Batch: 22, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 25, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 26, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 27, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 28, Loss: 0.01615419\n",
      "Epoch: 1, Batch: 29, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 30, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 31, Loss: 0.17992425\n",
      "Epoch: 1, Batch: 32, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 33, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 34, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 35, Loss: 0.05681818\n",
      "Epoch: 1, Batch: 37, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 38, Loss: 0.05291890\n",
      "Epoch: 1, Batch: 39, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 40, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 41, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 42, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 45, Loss: 0.00501337\n",
      "Epoch: 1, Batch: 46, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 47, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 48, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 49, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 50, Loss: 0.08077095\n",
      "Epoch: 1, Batch: 51, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 52, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 53, Loss: 0.04233512\n",
      "Epoch: 1, Batch: 54, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 55, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 57, Loss: 0.00557041\n",
      "Epoch: 1, Batch: 58, Loss: 0.01058378\n",
      "Epoch: 1, Batch: 59, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 60, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 61, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 62, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 63, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 64, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 65, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 66, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 67, Loss: 0.04901961\n",
      "Epoch: 1, Batch: 68, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 69, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 70, Loss: 0.04456328\n",
      "Epoch: 1, Batch: 71, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 72, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 73, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 74, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 76, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 77, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 78, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 79, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 80, Loss: 0.04400624\n",
      "Epoch: 1, Batch: 81, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 83, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 84, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 85, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 86, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 87, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 88, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 89, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 90, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 91, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 92, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 93, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 94, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 95, Loss: 0.30135918\n",
      "Epoch: 1, Batch: 96, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 97, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 98, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 99, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 100, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 101, Loss: 0.08578432\n",
      "Epoch: 1, Batch: 103, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 104, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 105, Loss: 0.01615419\n",
      "Epoch: 1, Batch: 107, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 108, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 109, Loss: 0.15262923\n",
      "Epoch: 1, Batch: 110, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 111, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 112, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 113, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 115, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 116, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 117, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 118, Loss: 0.00501337\n",
      "Epoch: 1, Batch: 119, Loss: 0.11140820\n",
      "Epoch: 1, Batch: 120, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 121, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 122, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 123, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 124, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 126, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 127, Loss: 0.11809269\n",
      "Epoch: 1, Batch: 128, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 129, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 130, Loss: 0.20833333\n",
      "Epoch: 1, Batch: 132, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 133, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 134, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 135, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 136, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 137, Loss: 0.01169786\n",
      "Epoch: 1, Batch: 138, Loss: 0.00946970\n",
      "Epoch: 1, Batch: 139, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 140, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 141, Loss: 0.00946970\n",
      "Epoch: 1, Batch: 142, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 143, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 145, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 146, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 147, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 148, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 152, Loss: 0.09135472\n",
      "Epoch: 1, Batch: 153, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 154, Loss: 0.07575758\n",
      "Epoch: 1, Batch: 156, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 157, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 158, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 159, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 160, Loss: 0.77707219\n",
      "Epoch: 1, Batch: 161, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 162, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 163, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 164, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 165, Loss: 0.01169786\n",
      "Epoch: 1, Batch: 166, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 167, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 168, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 169, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 170, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 171, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 172, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 173, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 174, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 175, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 176, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 177, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 178, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 179, Loss: 0.12032086\n",
      "Epoch: 1, Batch: 180, Loss: 0.00445633\n",
      "Epoch: 1, Batch: 181, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 182, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 183, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 184, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 185, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 186, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 187, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 188, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 189, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 190, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 191, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 193, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 194, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 195, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 196, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 197, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 198, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 199, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 200, Loss: 0.00612745\n",
      "Epoch: 1, Batch: 201, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 202, Loss: 0.18995099\n",
      "Epoch: 1, Batch: 203, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 204, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 205, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 207, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 208, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 209, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 211, Loss: 0.00445633\n",
      "Epoch: 1, Batch: 212, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 213, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 214, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 215, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 216, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 217, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 219, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 220, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 221, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 222, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 223, Loss: 0.00501337\n",
      "Epoch: 1, Batch: 224, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 225, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 226, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 227, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 228, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 229, Loss: 0.08299911\n",
      "Epoch: 1, Batch: 231, Loss: 0.05180481\n",
      "Epoch: 1, Batch: 232, Loss: 0.27852049\n",
      "Epoch: 1, Batch: 233, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 234, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 236, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 237, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 238, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 239, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 240, Loss: 0.00724153\n",
      "Epoch: 1, Batch: 241, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 242, Loss: 0.00557041\n",
      "Epoch: 1, Batch: 243, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 244, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 245, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 246, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 248, Loss: 0.03899287\n",
      "Epoch: 1, Batch: 249, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 250, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 251, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 252, Loss: 0.01114082\n",
      "Epoch: 1, Batch: 253, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 254, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 255, Loss: 0.44173351\n",
      "Epoch: 1, Batch: 257, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 258, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 259, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 260, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 261, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 262, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 263, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 264, Loss: 0.06740196\n",
      "Epoch: 1, Batch: 265, Loss: 0.26403743\n",
      "Epoch: 1, Batch: 266, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 267, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 268, Loss: 0.19440731\n",
      "Epoch: 1, Batch: 269, Loss: 0.19050802\n",
      "Epoch: 1, Batch: 270, Loss: 0.08689839\n",
      "Epoch: 1, Batch: 271, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 272, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 273, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 274, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 275, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 276, Loss: 0.02450980\n",
      "Epoch: 1, Batch: 277, Loss: 0.69295901\n",
      "Epoch: 1, Batch: 279, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 280, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 281, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 282, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 283, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 284, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 285, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 286, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 287, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 288, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 289, Loss: 0.00334225\n",
      "Epoch: 1, Batch: 290, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 291, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 292, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 293, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 294, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 295, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 296, Loss: 0.96256685\n",
      "Epoch: 1, Batch: 297, Loss: 0.00222816\n",
      "Epoch: 1, Batch: 298, Loss: 0.13146168\n",
      "Epoch: 1, Batch: 299, Loss: 0.09024064\n",
      "Epoch: 1, Batch: 300, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 301, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 302, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 303, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 304, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 305, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 306, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 308, Loss: 0.00724153\n",
      "Epoch: 1, Batch: 310, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 311, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 313, Loss: 0.10862300\n",
      "Epoch: 1, Batch: 314, Loss: 0.00389929\n",
      "Epoch: 1, Batch: 315, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 317, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 318, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 319, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 320, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 322, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 323, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 324, Loss: 0.26905081\n",
      "Epoch: 1, Batch: 325, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 326, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 328, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 329, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 330, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 331, Loss: 0.08912656\n",
      "Epoch: 1, Batch: 332, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 333, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 334, Loss: 0.00278520\n",
      "Epoch: 1, Batch: 335, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 336, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 337, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 338, Loss: 0.12143493\n",
      "Epoch: 1, Batch: 339, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 340, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 341, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 342, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 343, Loss: 0.14037433\n",
      "Epoch: 1, Batch: 344, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 345, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 346, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 347, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 348, Loss: 0.25401071\n",
      "Epoch: 1, Batch: 350, Loss: 0.41443852\n",
      "Epoch: 1, Batch: 351, Loss: 0.16989751\n",
      "Epoch: 1, Batch: 352, Loss: 0.00111408\n",
      "Epoch: 1, Batch: 353, Loss: 0.00167112\n",
      "Epoch: 1, Batch: 354, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 355, Loss: 0.00000000\n",
      "Epoch: 1, Batch: 356, Loss: 0.00055704\n",
      "Epoch: 1, Batch: 357, Loss: 0.24621212\n",
      "Epoch: 1, Batch: 358, Loss: 0.00000000\n",
      "Epoch 1/1 completed.\n",
      "Average train_loss: 0.02809447    Average train_accuracy: 0.99971968\n",
      "Average val_loss: 0.04200089    Average val_accuracy: 0.99958009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FireRiskConv(\n",
       "  (adapter): ChannelMlp(\n",
       "    (linear1): Conv2D(58, 64, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (linear2): Conv2D(64, 25, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (mlp): ChannelMlp(\n",
       "    (linear1): Conv2D(48, 128, kernel_size=[1, 1], data_format=NCHW)\n",
       "    (linear2): Conv2D(128, 58, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (segmenthead): BinarySegmentationHead(\n",
       "    (conv): Conv2D(58, 1, kernel_size=[1, 1], data_format=NCHW)\n",
       "  )\n",
       "  (nwp): Sequential(\n",
       "    (0): Conv2D(25, 36, kernel_size=[5, 5], padding=same, data_format=NCHW)\n",
       "    (1): Conv2D(36, 48, kernel_size=[5, 5], padding=same, data_format=NCHW)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import train_model\n",
    "\n",
    "train_model(mymodel_conv, times=3,\n",
    "            epochs=1, data_paths=data_paths, val_ratio=0.1,\n",
    "            lr_scheduler=0.031, lr_shedular_type='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57619c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:44:37.592412Z",
     "iopub.status.busy": "2025-11-18T18:44:37.591406Z",
     "iopub.status.idle": "2025-11-18T18:46:40.787405Z",
     "shell.execute_reply": "2025-11-18T18:46:40.778792Z",
     "shell.execute_reply.started": "2025-11-18T18:44:37.592412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2025/11/19 07:43:13] ppsci WARNING: Logger has already been automatically initialized as `log_file` is set to None by default, information will only be printed to terminal without writing to any file.\u001b[0m\n",
      "\u001b[1;36m[2025/11/19 07:43:13] ppsci MESSAGE: Inference with engine: native, precision: fp32, device: gpu.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "cfg = OmegaConf.create({'INFER':{'pdmodel_path':'C:/Users/chene/python programs/vscode_workspace_py/fire_risk/inference/eastern/yinglong_eastern.pdmodel',\n",
    "                                 'pdiparams_path':'C:/Users/chene/python programs/vscode_workspace_py/fire_risk/inference/eastern/yinglong_eastern.pdiparams',\n",
    "                                 'device':'gpu',\n",
    "                                 'engine':'native',\n",
    "                                 'precision':'fp32',\n",
    "                                 'onnx_path':'C:/Users/chene/python programs/vscode_workspace_py/fire_risk/inference/eastern/yinglong_eastern.onnx',\n",
    "                                 'ir_optim':False,\n",
    "                                 'min_subgraph_size':30,\n",
    "                                 'gpu_mem':150,\n",
    "                                 'gpu_id':0,\n",
    "                                 'max_batch_size':1,\n",
    "                                 'num_cpu_threads':15\n",
    "                                 }})\n",
    "\n",
    "from model import FireRiskYinglong\n",
    "\n",
    "mymodel = FireRiskYinglong(in_channels = 6*8+10,\n",
    "                   adapter_channels = 64,\n",
    "                   mlp_hidden_channels = 256,\n",
    "                   cfg=cfg)\n",
    "\n",
    "# 需要修改PaddleScience.deploy.python_infer>base.Predictor._create_paddle_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610770f-7773-4560-a376-62873dcaac85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fire_risk_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
